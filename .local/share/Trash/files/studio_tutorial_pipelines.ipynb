{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q scramp==1.2.2 awswrangler==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries including SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from time import gmtime, strftime\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "from demo_helper import ModelMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update SageMaker SDK if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if int(sagemaker.__version__.split('.')[0]) != 2:\n",
    "    !pip install sagemaker==2.24.1\n",
    "    print(\"Updating SageMakerVersion. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"SageMaker SDK version is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories in the SageMaker default bucket for this tutorial¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket= sess.default_bucket() # Alterantively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-tutorial' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'\n",
    "\n",
    "\n",
    "training_job_output_path = f's3://{default_bucket}/{prefix}/training_jobs'\n",
    "\n",
    "create_dataset_script_uri = f's3://{default_bucket}/{prefix}/code/create_dataset.py'\n",
    "deploy_model_script_uri = f's3://{default_bucket}/{prefix}/code/deploy_model.py'\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "\n",
    "#======> variables used for parameterizing the notebook run\n",
    "flow_instance_count = 1\n",
    "flow_instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippet to download the dataset to /data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data\n",
    "!wget -O ./data/default_of_credit_card.xls  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names and save the file as .csv\n",
    "data_path = './data/default_of_credit_card.xls'\n",
    "\n",
    "df = pd.read_excel('./data/default_of_credit_card.xls', header=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.to_datetime('now').timestamp()\n",
    "df['EVENT_TIME'] = timestamp\n",
    "\n",
    "df = df.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon SageMaker built-in XGBoost algorithm, the label column needs to be the first column in the dataframe. Use the code below to make that change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df)\n",
    "cols.insert(0, cols.pop(cols.index('default payment next month')))\n",
    "df = df.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"default payment next month\": \"LABEL\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to upload the data to S3. Also after running the cell below, you can continue the guide using the provided `./data/dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/dataset.csv', index=False)\n",
    "\n",
    "response = sess.upload_data('./data/dataset.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=.80, random_state=42)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "# save train and test data locally\n",
    "df_train.to_csv('./data/train.csv', index=False)\n",
    "df_test.to_csv('./data/test.csv', index=False)\n",
    "\n",
    "# upload train and test data to s3\n",
    "response = sagemaker_session.upload_data('./data/train.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "train_data_uri = response\n",
    "\n",
    "response = sagemaker_session.upload_data('./data/test.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "test_data_uri = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Feature Store\n",
    "\n",
    "Amazon SageMaker Feature Store is a purpose-built repository where you can store and access features so it’s much easier to name, organize, and reuse them across teams. SageMaker Feature Store provides a unified store for features during training and real-time inference without the need to write additional code or create manual processes to keep features consistent. SageMaker Feature Store keeps track of the metadata of stored features (e.g. feature name or version number) so that you can query the features for the right attributes in batches or in real time using Amazon Athena, an interactive query service. SageMaker Feature Store also keeps features updated, because as new data is generated during inference, the single repository is updated so new features are always available for models to use during training and inference.\n",
    "\n",
    "A feature store consists of an offline componet stored in S3 and an online component stored in a low-latency database. The online database is optional, but very useful if you need supplemental features to be available at inference. In this section, we will create a feature groups for our Claims and Customers datasets. After inserting the claims and customer data into their respective feature groups, you need to query the offline store with Athena to build the training dataset.\n",
    "\n",
    "You can reference the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) for more information about the SageMaker Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime', \n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the feature groups\n",
    "\n",
    "When you set up your feature groups, you need to customize the feature names with a unique name and set up each feature group with the FeatureGroup class. \n",
    "\n",
    "\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the correspongin Python data type in the Pandas dataframe when it's ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_name = f'credit-default' + strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "credit_feature_group = FeatureGroup(\n",
    "    name=fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "# You can now load the feature definitions by passing a data frame containing the feature data.\n",
    "credit_feature_group.load_feature_definitions(data_frame=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the feature groups\n",
    "\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record indentifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{fg_name} is the feature group name in use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you use the create function to create the feature group. The following code shows all of the available parameters. The online store is not created by default, so you must set this as True if you want to enable it. The s3_uri is the S3 bucket location of your offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = 'ID'\n",
    "event_time_feature_name = 'EVENT_TIME'\n",
    "\n",
    "\n",
    "# check if the feature groups is created successfully \n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "    \n",
    "\n",
    "print(f\"\\n Using s3://{default_bucket}/{prefix}\")\n",
    "credit_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=sagemaker_role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=credit_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'credit_table' in locals():\n",
    "    print(\"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\")\n",
    "\n",
    "else:\n",
    "    credit_feature_group.ingest(\n",
    "    data_frame=df, max_workers=5, wait=True\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "Now that youve manually done each step in our machine learning workflow, you can create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call pipeline.start(parameters=parameters) later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "dw_flow = './credit_default_flow/credit_default.flow'\n",
    "# # set an output path where the trained model will be saved\n",
    "# model_prefix = 'xgboost'\n",
    "# output_path = 's3://{}/{}/output'.format(default_bucket, prefix)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=train_instance_count, \n",
    "                                          instance_type=train_instance_type, \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=training_job_output_path)\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name='XgboostTrain',\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(train_data_uri, content_type=content_type)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Pre-Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name='credit-default-demo-pipeline-xgboost',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"ModelPreDeployment\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Run Bias Metrics with Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify config\n",
    "bias_report_output_path = f's3://{default_bucket}/{prefix}/clarify-output/bias'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "\n",
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    label='LABEL',\n",
    "    dataset_type='text/csv',\n",
    "s3_output_path=bias_report_output_path)\n",
    "\n",
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name='SEX',\n",
    "    facet_values_or_threshold=[1])\n",
    "\n",
    "analysis_config = bias_data_config.get_config()\n",
    "analysis_config.update(bias_config.get_config())\n",
    "analysis_config[\"methods\"] = {\"pre_training_bias\": {\"methods\": \"all\"}}\n",
    "\n",
    "clarify_config_dir = pathlib.Path('config')\n",
    "clarify_config_dir.mkdir(exist_ok=True)\n",
    "with open(clarify_config_dir / 'analysis_config.json', 'w') as f:\n",
    "    json.dump(analysis_config, f)\n",
    "    \n",
    "s3_client.upload_file(Filename='config/analysis_config.json', Bucket=default_bucket, Key=f'{prefix}/clarify-config/analysis_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify processing step\n",
    "\n",
    "clarify_processor = sagemaker.processing.Processor(\n",
    "    base_job_name='fraud-detection-demo-clarify-processor',\n",
    "    image_uri=sagemaker.clarify.image_uris.retrieve(framework='clarify', region=region),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge')\n",
    "\n",
    "clarify_step = ProcessingStep(\n",
    "    name=\"ClarifyProcessor\",\n",
    "    processor=clarify_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"analysis_config\",\n",
    "            source=f's3://{default_bucket}/{prefix}/clarify-config/analysis_config.json',\n",
    "            destination=\"/opt/ml/processing/input/config\"),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"dataset\",\n",
    "            source=train_data_uri,\n",
    "            destination=\"/opt/ml/processing/input/data\")  \n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output/analysis.json\",\n",
    "            destination=bias_report_output_path,\n",
    "            output_name=\"analysis_result\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    bias=sagemaker.model_metrics.MetricsSource(\n",
    "        s3_uri=clarify_step.properties.ProcessingOutputConfig.Outputs['analysis_result'].S3Output.S3Uri,\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if 'mpg_name' not in locals():\n",
    "    mpg_name = prefix\n",
    "    %store mpg_name\n",
    "    print(f'Model Package Group name: {mpg_name}')\n",
    "    \n",
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"xgboost-model-pipeline-0120\"\n",
    "\n",
    "s3_client.upload_file(Filename='deploy_model.py', Bucket=default_bucket, Key=f'{prefix}/code/deploy_model.py')\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name='fraud-detection-demo-deploy-model',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='DeployModel',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName, \n",
    "        \"--region\", region,\n",
    "        \"--endpoint-instance-type\", deploy_model_instance_type,\n",
    "        \"--endpoint-name\", endpoint_name],\n",
    "    code=deploy_model_script_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the Pipeline Steps and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f'credit-default'\n",
    "%store pipeline_name\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        train_step, \n",
    "        create_model_step, \n",
    "        clarify_step, \n",
    "        register_step,\n",
    "        deploy_step\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline definition to the SageMaker Pipeline service¶\n",
    "\n",
    "Note: If an existing pipeline has the same name it will be overwritten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start()\n",
    "\n",
    "start_response.wait()\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  View results of Clarify job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample an application from the test data and get it's features from online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = df_test.sample(1).iloc[0]['ID']\n",
    "\n",
    "response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=fg_name, \n",
    "        RecordIdentifierValueAsString= str(sample_id)\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.DataFrame(response['Record']).set_index('FeatureName').drop('LABEL')\n",
    "data_input = ','.join(df_sample['ValueAsString'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(data_input, initial_args = {\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "\n",
    "print (f'Probablitity of the default payment next month for ID:{int(sample_id)} is : {round(prediction * 100)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "After running the demo, you should remove the resources which were created. You can also delete all the objects in the project's S3 directory by passing the keyword argument delete_s3_objects=True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from demo_helper import delete_project_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_project_resources(\n",
    "#     sagemaker_boto_client=sagemaker_boto_client,\n",
    "#     endpoint_name=endpoint_name, \n",
    "#     pipeline_name=pipeline_name, \n",
    "#     mpg_name=mpg_name, \n",
    "#     prefix=prefix,\n",
    "#     delete_s3_objects=False,\n",
    "#     bucket_name=default_bucket)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
