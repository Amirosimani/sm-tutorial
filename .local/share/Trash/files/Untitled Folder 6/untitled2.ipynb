{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangler Export to SageMaker Pipelines Notebook\n",
    "\n",
    "You can use Amazon SageMaker Pipelines to create\n",
    "end-to-end workflows that manage and deploy SageMaker jobs. Pipelines\n",
    "come with SageMaker Python SDK integration, so you can build each step\n",
    "of your workflow using a Python-based interface.\n",
    "\n",
    "After your workflow is deployed, you can view the Directed Acyclic Graph\n",
    "(DAG) for your pipeline and manage your executions using Amazon SageMaker Studio.\n",
    "\n",
    "Use this notebook to create a SageMaker pipeline with a data preperation step,\n",
    "defined by your Data Wrangler flow.\n",
    "\n",
    "In this notebook, you will do the following:\n",
    "* Upload your Data Wrangler .flow file to S3 so that it can be used to define\n",
    "a processing job step.\n",
    "* Define a processing job step. This step is used to create a pipeline.\n",
    "* Define a pipeline that includes a data preperation steps defined by your\n",
    "Data Wrangler flow. Optionally, you can add additional steps to your pipeline.\n",
    "* Execute the pipeline and monitor its status using SageMaker Pipeline APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Installing dependencies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "original_version = pkg_resources.get_distribution(\"sagemaker\").version\n",
    "_ = subprocess.check_call(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "The following lists parameters that are used throughout this notebook.\n",
    "You can, optionally, use the following cell to configure these variables:\n",
    "* `bucket` - The S3 bucket used to save the output returned\n",
    "from the processing job and the flow file you exported from Data Wrangler.\n",
    "* `prefix` - This is the prefix your .flow file is saved under in S3.\n",
    "* `flow_id` and `flow_name` - used to name your flow file when it is saved\n",
    "to S3.\n",
    "* `instance_type` - The instance type used in your processing job.\n",
    "* `output_content_type` - The format type used to save the output of the\n",
    "processing job.\n",
    "* `sagemaker_endpoint_url` - An endpoint URL used to communicate with SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 bucket and location used to save processing job outputs and your .flow file.\n",
    "# Specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "\n",
    "# Do not modify flow_file_name\n",
    "flow_file_name = \"untitled.flow\"\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = \"174368400705.dkr.ecr.us-west-2.amazonaws.com/sagemaker-data-wrangler-container:1.2.1\"\n",
    "\n",
    "# Set pipeline_deletion to True if you would like to clean up pipelines created in this notebook\n",
    "pipeline_deletion = False\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "# Data wrangler processing job only supports 1 instance.\n",
    "instance_count = 1\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Processing Job Path URI Information\n",
    "output_prefix = f\"export-{flow_name}/output\"\n",
    "output_path = f\"s3://{bucket}/{output_prefix}\"\n",
    "output_name = \"d5e69256-194c-4f2c-86e9-a80c2a18b79a.default\"\n",
    "\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# Modify the variable below to specify the content type to be used for writing each output\n",
    "# Currently supported options are 'CSV' or 'PARQUET', and the default is 'CSV'\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use\n",
    "# when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None\n",
    "\n",
    "add_training_step = False"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Upload the Data Wrangler .flow file to Amazon S3 so that it can be used as an input to the\n",
    "processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .flow file\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/{flow_name}.flow\")\n",
    "\n",
    "print(f\"Data Wrangler Flow uploaded to {flow_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Processing Job arguments\n",
    "\n",
    "This notebook submits a processing job using the SageMaker Python SDK, which create an argument dictionary to\n",
    "submit to the underlying boto client. Below, utility methods are defined for creating processing job inputs\n",
    "for the following sources: S3, Athena, and Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "\n",
    "def create_flow_notebook_processing_input(base_dir, flow_s3_uri):\n",
    "    return ProcessingInput(\n",
    "        source=flow_s3_uri,\n",
    "        destination=f\"{base_dir}/flow\",\n",
    "        input_name=\"flow\",\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_s3_processing_input(s3_dataset_definition, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        source=s3_dataset_definition['s3ExecutionContext']['s3Uri'],\n",
    "        destination=f\"{base_dir}/{name}\",\n",
    "        input_name=name,\n",
    "        s3_data_type=\"S3Prefix\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"FullyReplicated\",\n",
    "    )\n",
    "\n",
    "\n",
    "def create_athena_processing_input(athena_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=athena_dataset_defintion[\"catalogName\"],\n",
    "                database=athena_dataset_defintion[\"databaseName\"],\n",
    "                query_string=athena_dataset_defintion[\"queryString\"],\n",
    "                output_s3_uri=athena_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=athena_dataset_defintion[\"outputFormat\"].upper()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_redshift_processing_input(redshift_dataset_defintion, name, base_dir):\n",
    "    return ProcessingInput(\n",
    "        input_name=name,\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=f\"{base_dir}/{name}\",\n",
    "            redshift_dataset_definition=RedshiftDatasetDefinition(\n",
    "                cluster_id=redshift_dataset_defintion[\"clusterIdentifier\"],\n",
    "                database=redshift_dataset_defintion[\"database\"],\n",
    "                db_user=redshift_dataset_defintion[\"dbUser\"],\n",
    "                query_string=redshift_dataset_defintion[\"queryString\"],\n",
    "                cluster_role_arn=redshift_dataset_defintion[\"unloadIamRole\"],\n",
    "                output_s3_uri=redshift_dataset_defintion[\"s3OutputLocation\"] + f\"{name}/\",\n",
    "                output_format=redshift_dataset_defintion[\"outputFormat\"].upper()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def create_processing_inputs(processing_dir, flow, flow_uri):\n",
    "    processing_inputs = []\n",
    "    flow_processing_input = create_flow_notebook_processing_input(processing_dir, flow_uri)\n",
    "    processing_inputs.append(flow_processing_input)\n",
    "\n",
    "    for node in flow[\"nodes\"]:\n",
    "        if \"dataset_definition\" in node[\"parameters\"]:\n",
    "            data_def = node[\"parameters\"][\"dataset_definition\"]\n",
    "            name = data_def[\"name\"]\n",
    "            source_type = data_def[\"datasetSourceType\"]\n",
    "\n",
    "            if source_type == \"S3\":\n",
    "                processing_inputs.append(create_s3_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Athena\":\n",
    "                processing_inputs.append(create_athena_processing_input(data_def, name, processing_dir))\n",
    "            elif source_type == \"Redshift\":\n",
    "                processing_inputs.append(create_redshift_processing_input(data_def, name, processing_dir))\n",
    "            else:\n",
    "                raise ValueError(f\"{source_type} is not supported for Data Wrangler Processing.\")\n",
    "\n",
    "    return processing_inputs\n",
    "\n",
    "\n",
    "def create_processing_output(output_name, output_path, processing_dir):\n",
    "    return ProcessingOutput(\n",
    "        output_name=output_name,\n",
    "        source=os.path.join(processing_dir, \"output\"),\n",
    "        destination=output_path,\n",
    "        s3_upload_mode=\"EndOfJob\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type\n",
    "        }\n",
    "    }\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The following cell creates a processing step using your exported Data Wrangler flow.\n",
    "This step will be used to create a SageMaker Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    max_runtime_in_seconds=86400\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DataWranglerProcessingStep\",\n",
    "    processor=processor,\n",
    "    inputs=create_processing_inputs(processing_dir, flow, flow_uri),\n",
    "    outputs=[create_processing_output(output_name, output_path, processing_dir)],\n",
    "    job_arguments=create_container_arguments(output_name, output_content_type)\n",
    ")\n",
    "\n",
    "step_list = [step_process]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The ProcessingStep above is included in the step list, which will be used to create the Pipeline.\n",
    "Most users will want to add additional steps to their Pipeline. The below shows an example of a\n",
    "training step; include step_train in the step list to include it in the Pipeline created next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=instance_type,\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    role=iam_role,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:squarederror\",\n",
    "    num_round=3,\n",
    ")\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "xgb_input_content_type = None\n",
    "\n",
    "if output_content_type == \"CSV\":\n",
    "    xgb_input_content_type = 'text/csv'\n",
    "elif output_content_type == \"Parquet\":\n",
    "    xgb_input_content_type = 'application/x-parquet'\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"DataWrangerTrain\",\n",
    "    estimator=xgb_train,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                output_name\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=xgb_input_content_type\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Edit this to include the training step in the Pipeline!\n",
    "if add_training_step:\n",
    "    step_list.append(step_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Workflow Creation\n",
    "The following cell defines a new pipeline with the processing step.\n",
    "Use this cell to add additional steps to the pipeline. To learn more about adding\n",
    "steps to a pipeline, see\n",
    "[Define a Pipeline](http://docs.aws.amazon.com/sagemaker/latest/dg/define-pipeline.html)\n",
    "in the SageMaker documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"datawrangler-pipeline-{time.strftime('%m-%d-%H-%M', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "instance_type = ParameterString(name=\"InstanceType\", default_value=\"ml.m5.4xlarge\")\n",
    "instance_count = ParameterInteger(name=\"InstanceCount\", default_value=1)\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "runtime_client = boto_session.client(\"sagemaker-runtime\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_runtime_client=runtime_client,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[instance_type, instance_count],\n",
    "    steps=step_list,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run the following to validate the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Pipeline\n",
    "Use the following cell to submit a pipeline creation job.\n",
    "You can check the progress of the pipeline with the pipeline Amazon Resource Name (ARN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError, ValidationError\n",
    "\n",
    "try:\n",
    "    response = pipeline.create(role_arn=iam_role)\n",
    "except ClientError as e:\n",
    "    error = e.response[\"Error\"]\n",
    "    if error[\"Code\"] == \"ValidationError\" and \"Pipeline names must be unique\" in error[\"Message\"]:\n",
    "        print(error[\"Message\"])\n",
    "        response = pipeline.describe()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline Operations: Examine and Wait for Pipeline Execution\n",
    "\n",
    "The higher-level resources of the pipeline instance provide a way for the Data Scientist and\n",
    "Machine Learning Engineer to define a workflow that can be executed by SageMaker.\n",
    "\n",
    "To monitor operations of this execution, we use the lower-level, raw workflow boto3 client of the\n",
    "pipeline to describe the pipeline execution and list the pipeline execution steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start()\n",
    "pipeline_execution_arn = start_response.arn\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pipeline Status\n",
    "You can use the function [describe_pipeline_execution][1] to monitor a pipeline's execution.\n",
    "to view a pipeline's execution status.To view a pipeline's execution steps, you can use\n",
    "[list_pipeline_execution_steps][2].The following cell checks the pipeline status and execution\n",
    "steps using these functions.\n",
    "\n",
    "[1]: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribePipelineExecution.html\n",
    "[2]: https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ListPipelineExecutionSteps.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_response = sagemaker_session.sagemaker_client.describe_pipeline_execution(\n",
    "    PipelineExecutionArn=pipeline_execution_arn\n",
    ")\n",
    "print(\"Pipeline: {}.\".format(execution_response[\"PipelineExecutionStatus\"]))\n",
    "print()\n",
    "\n",
    "execution_steps_response = sagemaker_session.sagemaker_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=pipeline_execution_arn\n",
    ")\n",
    "execution_steps = execution_steps_response[\"PipelineExecutionSteps\"]\n",
    "print(\"Execution steps:\")\n",
    "pprint(execution_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the following cells to define and run a function that waits until the pipeline execution status\n",
    "changes to a terminal state: `Failed` or `Succeeded`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore.waiter\n",
    "\n",
    "def get_waiter(pipeline, delay=24, max_attempts=60):\n",
    "    waiter_id = \"PipelineExecutionComplete\"\n",
    "    model = botocore.waiter.WaiterModel({\n",
    "        \"version\": 2,\n",
    "        \"waiters\": {\n",
    "            waiter_id: {\n",
    "                \"delay\": delay,\n",
    "                \"maxAttempts\": max_attempts,\n",
    "                \"operation\": 'DescribePipelineExecution',\n",
    "                \"acceptors\": [\n",
    "                    {\n",
    "                        \"expected\": \"Succeeded\",\n",
    "                        \"matcher\": \"path\",\n",
    "                        \"state\": \"success\",\n",
    "                        \"argument\": \"PipelineExecutionStatus\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"expected\": \"Failed\",\n",
    "                        \"matcher\": \"path\",\n",
    "                        \"state\": \"failure\",\n",
    "                        \"argument\": \"PipelineExecutionStatus\"\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    return botocore.waiter.create_waiter_with_client(\n",
    "        waiter_id, model, sagemaker_session.sagemaker_client\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiter = get_waiter(pipeline)\n",
    "waiter.wait(PipelineExecutionArn=pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_steps_response = sagemaker_session.sagemaker_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=pipeline_execution_arn\n",
    ")\n",
    "execution_steps = execution_steps_response[\"PipelineExecutionSteps\"]\n",
    "print(\"Execution steps:\")\n",
    "pprint(execution_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleanup\n",
    "Uncomment the following code cell to revert the SageMaker Python SDK to the original version used\n",
    "before running this notebook. This notebook upgrades the SageMaker Python SDK to 2.x, which may\n",
    "cause other example notebooks to break. To learn more about the changes introduced in the\n",
    "SageMaker Python SDK 2.x update, see\n",
    "[Use Version 2.x of the SageMaker Python SDK.](https://sagemaker.readthedocs.io/en/stable/v2.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = subprocess.check_call(\n",
    "#         [sys.executable, \"-m\", \"pip\", \"install\", f\"sagemaker=={original_version}\"]\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pipeline cleanup\n",
    "Set `pipeline_deletion` flag to `True` to delete the SageMaker Pipelines created\n",
    " in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipeline_deletion:\n",
    "    client = boto3.client('sagemaker')\n",
    "    client.delete_pipeline(PipelineName=pipeline_name)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
