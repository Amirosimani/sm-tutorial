{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q scramp==1.2.2 awswrangler==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries including SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from time import gmtime, strftime\n",
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update SageMaker SDK if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker SDK version is good\n"
     ]
    }
   ],
   "source": [
    "if int(sagemaker.__version__.split('.')[0]) != 2:\n",
    "    !pip install sagemaker==2.24.1\n",
    "    print(\"Updating SageMakerVersion. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"SageMaker SDK version is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region = us-west-2\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories in the SageMaker default bucket for this tutorial¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket= sess.default_bucket() # Alterantively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-tutorial' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippet to download the dataset to /data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\n",
      "--2021-03-04 19:43:11--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
      "Saving to: ‘./data/default_of_credit_card.xls’\n",
      "\n",
      "./data/default_of_c 100%[===================>]   5.28M  19.1MB/s    in 0.3s    \n",
      "\n",
      "2021-03-04 19:43:12 (19.1 MB/s) - ‘./data/default_of_credit_card.xls’ saved [5539328/5539328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data\n",
    "!wget -O ./data/default_of_credit_card.xls  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change column names and save the file as .csv\n",
    "data_path = './data/default_of_credit_card.xls'\n",
    "\n",
    "df = pd.read_excel('./data/default_of_credit_card.xls', header=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                            0\n",
       "LIMIT_BAL                     0\n",
       "SEX                           0\n",
       "EDUCATION                     0\n",
       "MARRIAGE                      0\n",
       "AGE                           0\n",
       "PAY_0                         0\n",
       "PAY_2                         0\n",
       "PAY_3                         0\n",
       "PAY_4                         0\n",
       "PAY_5                         0\n",
       "PAY_6                         0\n",
       "BILL_AMT1                     0\n",
       "BILL_AMT2                     0\n",
       "BILL_AMT3                     0\n",
       "BILL_AMT4                     0\n",
       "BILL_AMT5                     0\n",
       "BILL_AMT6                     0\n",
       "PAY_AMT1                      0\n",
       "PAY_AMT2                      0\n",
       "PAY_AMT3                      0\n",
       "PAY_AMT4                      0\n",
       "PAY_AMT5                      0\n",
       "PAY_AMT6                      0\n",
       "default payment next month    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = df.drop(columns=['income', 'native-country'])\n",
    "\n",
    "# # removing trailing and heading whitespaces in column values\n",
    "# string_cols = df_features.select_dtypes(['object']).columns\n",
    "# df_features[string_cols] = df_features[string_cols].apply(lambda x: x.str.strip())\n",
    "\n",
    "# # remove special characters from native-country column\n",
    "# # df_features['native-country'] = df_features['native-country'].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x))\n",
    "\n",
    "\n",
    "# # one hot encoding is applied to prepare the data for feature store ingestion\n",
    "# df_features = pd.get_dummies(df_features)\n",
    "\n",
    "# df_features = df_features.fillna(0)\n",
    "# df_features = df_features.reset_index()\n",
    "\n",
    "timestamp = pd.to_datetime('now').timestamp()\n",
    "df['EVENT_TIME'] = timestamp\n",
    "\n",
    "# df['ID'] =  pd.to_numeric(df['ID'] , downcast='float64')\n",
    "# df['ID'] =df['ID'].astype(float)\n",
    "# df['ID'] = df['ID'].apply(np.uint64)\n",
    "\n",
    "df = df.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon SageMaker built-in XGBoost algorithm, the label column needs to be the first column in the dataframe. Use the code below to make that change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df)\n",
    "cols.insert(0, cols.pop(cols.index('default payment next month')))\n",
    "df = df.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"default payment next month\": \"LABEL\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to upload the data to S3. Also after running the cell below, you can continue the guide using the provided `./data/dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial/data/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('./data/dataset.csv', index=False)\n",
    "\n",
    "response = sess.upload_data('./data/dataset.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=.80, random_state=42)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "# save train and test data locally\n",
    "df_train.to_csv('./data/train.csv', index=False)\n",
    "df_test.to_csv('./data/test.csv', index=False)\n",
    "\n",
    "# upload train and test data to s3\n",
    "response = sess.upload_data('./data/train.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "train_data_uri = response\n",
    "\n",
    "response = sess.upload_data('./data/test.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "test_data_uri = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Feature Store is a purpose-built repository where you can store and access features so it’s much easier to name, organize, and reuse them across teams. SageMaker Feature Store provides a unified store for features during training and real-time inference without the need to write additional code or create manual processes to keep features consistent. SageMaker Feature Store keeps track of the metadata of stored features (e.g. feature name or version number) so that you can query the features for the right attributes in batches or in real time using Amazon Athena, an interactive query service. SageMaker Feature Store also keeps features updated, because as new data is generated during inference, the single repository is updated so new features are always available for models to use during training and inference.\n",
    "\n",
    "A feature store consists of an offline componet stored in S3 and an online component stored in a low-latency database. The online database is optional, but very useful if you need supplemental features to be available at inference. In this section, we will create a feature groups for our Claims and Customers datasets. After inserting the claims and customer data into their respective feature groups, you need to query the offline store with Athena to build the training dataset.\n",
    "\n",
    "You can reference the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) for more information about the SageMaker Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime', \n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "\n",
    "When you set up your feature groups, you need to customize the feature names with a unique name and set up each feature group with the FeatureGroup class. \n",
    "\n",
    "\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the correspongin Python data type in the Pandas dataframe when it's ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_name = f'credit-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "credit_feature_group = FeatureGroup(\n",
    "    name=fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "# You can now load the feature definitions by passing a data frame containing the feature data.\n",
    "credit_feature_group.load_feature_definitions(data_frame=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups¶\n",
    "\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record indentifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit-04-20-45-51 is the feature group name in use\n"
     ]
    }
   ],
   "source": [
    "print(f\"{fg_name} is the feature group name in use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you use the create function to create the feature group. The following code shows all of the available parameters. The online store is not created by default, so you must set this as True if you want to enable it. The s3_uri is the S3 bucket location of your offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Using s3://{default_bucket}/{prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup credit-04-20-45-51 successfully created.\n"
     ]
    }
   ],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = 'ID'\n",
    "event_time_feature_name = 'EVENT_TIME'\n",
    "\n",
    "\n",
    "# check if the feature groups is created successfully \n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "    \n",
    "\n",
    "print(f\"\\n Using s3://{default_bucket}/{prefix}\")\n",
    "credit_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=sagemaker_role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=credit_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-west-2:367158743199:feature-group/credit-04-20-45-51',\n",
       " 'FeatureGroupName': 'credit-04-20-45-51',\n",
       " 'RecordIdentifierFeatureName': 'ID',\n",
       " 'EventTimeFeatureName': 'EVENT_TIME',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'LABEL', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'ID', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'LIMIT_BAL', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'SEX', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'EDUCATION', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'MARRIAGE', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'AGE', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_0', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT1', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT1', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'EVENT_TIME', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 3, 4, 20, 46, 2, 387000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-west-2-367158743199/sagemaker-tutorial'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'credit-04-20-45-51-1614890762',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'RoleArn': 'arn:aws:iam::367158743199:role/service-role/AmazonSageMaker-ExecutionRole-20210217T120320',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'ResponseMetadata': {'RequestId': '49bafada-6560-4422-8071-dec77065b643',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '49bafada-6560-4422-8071-dec77065b643',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2635',\n",
       "   'date': 'Thu, 04 Mar 2021 20:46:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'credit_table' in locals():\n",
    "    print(\"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\")\n",
    "\n",
    "else:\n",
    "    credit_feature_group.ingest(\n",
    "    data_frame=df, max_workers=5, wait=True\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost built-in algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and test datasets have been persisted in S3, you can start training a model by defining which SageMaker Estimator you'd like to use. For this guide, you will use the XGBoost built-in algorithm to build an XGBoost training container as shown in the following code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "model_prefix = 'xgboost'\n",
    "output_path = 's3://{}/{}/output'.format(default_bucket, prefix)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"\n",
    "train_input = TrainingInput(train_data_uri, content_type=content_type)\n",
    "validation_input = TrainingInput(test_data_uri, content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lineage with artifacts and associations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker ML Lineage Tracking creates and stores information about the steps of a machine learning (ML) workflow from data preparation to model deployment. With the tracking information you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards. With SageMaker Lineage Tracking data scientists and model builders can do the following:\n",
    "\n",
    "* Keep a running history of model discovery experiments.\n",
    "* Establish model governance by tracking model lineage artifacts for auditing and compliance verification.\n",
    "* Clone and rerun workflows to experiment with what-if scenarios while developing models.\n",
    "* Share a workflow that colleagues can reproduce and enhance (for example, while collaborating on solving a business problem).\n",
    "* Clone and rerun workflows with additional debugging or logging routines, or new input variations for troubleshooting issues in production models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the xgb_estimator object retains much the data we need to learn about how the model was trained, it is, in fact, an ephermeral object which SageMaker does not persist and cannot be re-instantiated at a later time. Although we lose some of its convieneces once it is gone, we can still get back all the data we need by accessing the training jobs it once created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_1_info = sagemaker_boto_client.describe_training_job(TrainingJobName=estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_s3_uri = training_job_1_info['InputDataConfig'][0]['DataSource']['S3DataSource']['S3Uri']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=training_data_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {training_data_artifact.artifact_arn}')\n",
    "else:\n",
    "    training_data_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainingData',\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type='Dataset',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_s3_uri = training_job_1_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=trained_model_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {model_artifact.artifact_arn}')\n",
    "else:\n",
    "    model_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainedModel',\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type='Model',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {model_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set artifact associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_1_name = training_job_1_info['TrainingJobName']\n",
    "trial_component = sagemaker_boto_client.describe_trial_component(TrialComponentName=training_job_1_name+'-aws-training-job')\n",
    "trial_component_arn = trial_component['TrialComponentArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_artifacts = [training_data_artifact]\n",
    "\n",
    "for a in input_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='ContributedTo',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Association with {a.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {a.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_artifacts = [model_artifact]\n",
    "\n",
    "for a in output_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='Produced',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Association with {a.artifact_type}: SUCCESSFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {a.artifact_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model for bias with Clarify\n",
    "\n",
    "Amazon SageMaker Clarify helps improve your machine learning (ML) models by detecting potential bias and helping explain the predictions that models make. It helps you identify various types of bias in pretraining data and in posttraining that can emerge during model training or when the model is in production. SageMaker Clarify helps explain how these models make predictions using a feature attribution approach. It also monitors inferences models make in production for bias or feature attribution drift. The fairness and explainability functionality provided by SageMaker Clarify provides components that help AWS customers build less biased and more understandable machine learning models. It also provides tools to help you generate model governance reports which you can use to inform risk and compliance teams, and external regulators.\n",
    "\n",
    "You can reference the SageMaker Developer Guide for more information about SageMaker Clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model from estimator\n",
    "\n",
    "model_1_name = f'{prefix}-xgboost-pre-smote'\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_1_name)['Models']\n",
    "\n",
    "if not model_matches:\n",
    "    \n",
    "    model_1 = sagemaker_session.create_model_from_job(\n",
    "        name=model_1_name,\n",
    "        training_job_name=training_job_1_info['TrainingJobName'],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_1_info['AlgorithmSpecification']['TrainingImage'])\n",
    "else:\n",
    "    \n",
    "    print(f\"Model {model_1_name} already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for data set bias and model bias\n",
    "With SageMaker, we can check for pre-training and post-training bias. Pre-training metrics show pre-existing bias in that data, while post-training metrics show bias in the predictions from the model. Using the SageMaker SDK, we can specify which groups we want to check bias across and which metrics we'd like to show.\n",
    "\n",
    "To run the full Clarify job, you must un-comment the code in the cell below. Running the job will take ~15 minutes. If you wish to save time, you can view the results in the next cell after which loads a pre-generated output if no bias job was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_report_1_output_path = f's3://{default_bucket}/{prefix}/clarify-output/bias_1'\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "\n",
    "train_cols = wr.s3.read_csv(training_data_s3_uri).columns.to_list()\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=train_instance_type,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    s3_output_path=bias_report_1_output_path,\n",
    "    label='LABEL',\n",
    "    headers=train_cols,\n",
    "    dataset_type='text/csv')\n",
    "\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=model_1_name,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=1,\n",
    "    accept_type='text/csv',\n",
    "    content_type='text/csv')\n",
    "\n",
    "predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `BiasConfig` to provide information on which columns contain the facets (sensitive groups, Sex), what the sensitive features (facet_values_or_threshold) might be, and what the desirable outcomes are (label_values_or_threshold).\n",
    "\n",
    "You can run both the pretraining and posttraining analysis in the processing job at the same time with run_bias()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[2],\n",
    "    facet_name='SEX',\n",
    "    facet_values_or_threshold=[1])\n",
    "\n",
    "\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods='all',\n",
    "    post_training_methods='all')\n",
    "\n",
    "clarify_bias_job_1_name = clarify_processor.latest_job.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View results of Clarify job\n",
    "Running Clarify on your dataset or model can take ~15 minutes. Once it is done, you can view the results in Studio or download them from the bias_report_output_path S3 bucket.\n",
    "\n",
    "If you don't have time to run the job, you can view the pre-generated results included with this demo. Otherwise, you can run the job by un-commenting the code in the cell above.\n",
    "\n",
    "{PLACEHOLDER https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clarify_bias_job_name' in locals():\n",
    "    s3_client.download_file(Bucket=bucket, Key=f'{prefix}/clarify-output/bias-1/analysis.json', Filename='clarify_output/bias_1/analysis.json')\n",
    "    print(f'Downloaded analysis from previous Clarify job: {clarify_bias_job_name}')\n",
    "else:\n",
    "    print(f'Loading pre-generated analysis file...')\n",
    "\n",
    "with open('clarify_output/bias_1/analysis.json', 'r') as f:\n",
    "        bias_analysis = json.load(f)\n",
    "\n",
    "results = bias_analysis['pre_training_bias_metrics']['facets']['customer_gender_female'][0]['metrics'][1]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deposit Model and Lineage in SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained and its artifacts properly associated, the next step is to save the model in a registry for future reference and possible deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package Group\n",
    "A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mpg_name' (str)\n",
      "Model Package Group name: sagemaker-tutorial\n"
     ]
    }
   ],
   "source": [
    "if 'mpg_name' not in locals():\n",
    "    mpg_name = prefix\n",
    "    %store mpg_name\n",
    "    print(f'Model Package Group name: {mpg_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageGroupDescription': 'Insurance claim fraud detection'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)['ModelPackageGroupSummaryList']\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f'Using existing Model Package Group: {mpg_name}')\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f'Create Model Package Group {mpg_name}: SUCCESSFUL')\n",
    "    %store mpg_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package for trained model¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload a metrics report¶\n",
    "\n",
    "model_metrics_report = {'classification_metrics': {}}\n",
    "for metric in training_job_1_info['FinalMetricDataList']:\n",
    "    stat = {metric['MetricName']: {'value': metric['Value']}}\n",
    "    model_metrics_report['classification_metrics'].update(stat)\n",
    "    \n",
    "with open('./data/training_metrics.json', 'w') as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "    \n",
    "metrics_prefix = f\"{prefix}/training_jobs/{training_job_1_info['TrainingJobName']}\"\n",
    "\n",
    "\n",
    "sess.upload_data('./data/training_metrics.json', bucket=default_bucket, key_prefix=metrics_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the inference spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inf import InferenceSpecification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_1_info['AlgorithmSpecification']['TrainingImage'],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=['text/csv'],\n",
    "    supported_mime_types=['text/csv'])\n",
    "\n",
    "\n",
    "mp_inference_spec['InferenceSpecification']['Containers'][0]['ModelDataUrl'] = training_job_1_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'ModelQuality': {\n",
    "        'Statistics': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f's3://{default_bucket}/{prefix}/{metrics_s3_key}'\n",
    "        }\n",
    "    },\n",
    "    'Bias': {\n",
    "        'Report': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f'{bias_report_1_output_path}/analysis.json'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageDescription': 'XGBoost classifier to detect insurance fraud.',\n",
    "    'ModelApprovalStatus': 'PendingManualApproval',\n",
    "    'ModelMetrics': model_metrics\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp1_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp1_response['ModelPackageArn'])\n",
    "mp_status = mp_info['ModelPackageStatus']\n",
    "\n",
    "while mp_status not in ['Completed', 'Failed']:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp1_response['ModelPackageArn'])\n",
    "    mp_status = mp_info['ModelPackageStatus']\n",
    "    print(f'model package status: {mp_status}')\n",
    "print(f'model package status: {mp_status}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_list = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)['ModelPackageSummaryList']\n",
    "mp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve the model\n",
    "\n",
    "\n",
    "In the real-life MLOps lifecycle, a model package gets approved after evaluation by data scientists, subject matter experts and auditors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_update = {\n",
    "    'ModelPackageArn': mp_list[0]['ModelPackageArn'],\n",
    "    'ModelApprovalStatus': 'Approved'\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an inference endpoint\n",
    "\n",
    "Deploy an approved model to make prediction via Feature Store. This might take about  10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_container = {'ModelPackageName': mp_list[0]['ModelPackageArn']}\n",
    "endpoint_name = f'{model_1_name}-endpoint'\n",
    "endpoint_config_name=f'{model_1_name}-endpoint-config'\n",
    "existing_configs = len(sagemaker_boto_client.list_endpoint_configs(NameContains=endpoint_config_name, MaxResults = 30)['EndpointConfigs'])\n",
    "\n",
    "endpoint_instance_count = 1\n",
    "endpoint_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "if existing_configs == 0:\n",
    "    create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[{\n",
    "            'InstanceType': endpoint_instance_type,\n",
    "            'InitialVariantWeight': 1,\n",
    "            'InitialInstanceCount': endpoint_instance_count,\n",
    "            'ModelName': model_1_name,\n",
    "            'VariantName': 'AllTraffic'\n",
    "        }]\n",
    "    )\n",
    "    %store endpoint_config_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_endpoints = sagemaker_boto_client.list_endpoints(NameContains=endpoint_name, MaxResults = 30)['Endpoints']\n",
    "if not existing_endpoints:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        EndpointConfigName=endpoint_config_name)\n",
    "    %store endpoint_name\n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = endpoint_info['EndpointStatus']\n",
    "\n",
    "while endpoint_status == 'Creating':\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = endpoint_info['EndpointStatus']\n",
    "    print('Endpoint status:', endpoint_status)\n",
    "    if endpoint_status == 'Creating':\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample an application from the test data and get it's features from online feature store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = df_test.sample(1).iloc[0]['ID']\n",
    "sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = featurestore_runtime.get_record(\n",
    "        FeatureGroupName=fg_name, \n",
    "        RecordIdentifierValueAsString= str(sample_id)\n",
    "\n",
    "    )\n",
    "df_sample = pd.DataFrame(response['Record']).set_index('FeatureName').drop('LABEL')\n",
    "\n",
    "data_input = ','.join(df_sample['ValueAsString'])\n",
    "data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predictor.predict(data_input, initial_args = {\"ContentType\": \"text/csv\"})\n",
    "prediction = json.loads(results)\n",
    "\n",
    "print (f'Probablitity of the default payment next month for ID:{int(sample_id)} is :', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "Now that youve manually done each step in our machine learning workflow, you can create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call pipeline.start(parameters=parameters) later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "model_prefix = 'xgboost'\n",
    "output_path = 's3://{}/{}/output'.format(default_bucket, prefix)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "content_type = \"text/csv\"\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=train_instance_count, \n",
    "                                          instance_type=train_instance_type, \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name='XgboostTrain',\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(train_data_uri, content_type=content_type)\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pre-Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name='credit-default-demo-pipeline-xgboost',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"ModelPreDeployment\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bias Metrics with Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify config\n",
    "bias_report_output_path = f's3://{default_bucket}/{prefix}/clarify-output/bias'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "\n",
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    label='LABEL',\n",
    "    dataset_type='text/csv',\n",
    "s3_output_path=bias_report_output_path)\n",
    "\n",
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name='SEX',\n",
    "    facet_values_or_threshold=[1])\n",
    "\n",
    "analysis_config = bias_data_config.get_config()\n",
    "analysis_config.update(bias_config.get_config())\n",
    "analysis_config[\"methods\"] = {\"pre_training_bias\": {\"methods\": \"all\"}}\n",
    "\n",
    "clarify_config_dir = pathlib.Path('config')\n",
    "clarify_config_dir.mkdir(exist_ok=True)\n",
    "with open(clarify_config_dir / 'analysis_config.json', 'w') as f:\n",
    "    json.dump(analysis_config, f)\n",
    "    \n",
    "s3_client.upload_file(Filename='config/analysis_config.json', Bucket=default_bucket, Key=f'{prefix}/clarify-config/analysis_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify processing step\n",
    "\n",
    "clarify_processor = sagemaker.processing.Processor(\n",
    "    base_job_name='fraud-detection-demo-clarify-processor',\n",
    "    image_uri=sagemaker.clarify.image_uris.retrieve(framework='clarify', region=region),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge')\n",
    "\n",
    "clarify_step = ProcessingStep(\n",
    "    name=\"ClarifyProcessor\",\n",
    "    processor=clarify_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"analysis_config\",\n",
    "            source=f's3://{default_bucket}/{prefix}/clarify-config/analysis_config.json',\n",
    "            destination=\"/opt/ml/processing/input/config\"),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"dataset\",\n",
    "            source=train_data_uri,\n",
    "            destination=\"/opt/ml/processing/input/data\")  \n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output/analysis.json\",\n",
    "            destination=bias_report_output_path,\n",
    "            output_name=\"analysis_result\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_metrics = demo_helpers.ModelMetrics(\n",
    "#     bias=sagemaker.model_metrics.MetricsSource(\n",
    "#         s3_uri=clarify_step.properties.ProcessingOutputConfig.Outputs['analysis_result'].S3Output.S3Uri,\n",
    "#         content_type=\"application/json\"\n",
    "#     )\n",
    "# )\n",
    "\n",
    "if 'mpg_name' not in locals():\n",
    "    mpg_name = prefix\n",
    "    %store mpg_name\n",
    "    print(f'Model Package Group name: {mpg_name}')\n",
    "    \n",
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    "#     model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the Pipline Steps and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = f'credit-default'\n",
    "%store pipeline_name\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param, \n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "#         claims_flow_step,\n",
    "#         customers_flow_step,\n",
    "#         create_dataset_step,\n",
    "        train_step, \n",
    "        create_model_step, \n",
    "        clarify_step, \n",
    "        register_step,\n",
    "#         deploy_step\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline definition to the SageMaker Pipeline service¶\n",
    "\n",
    "Note: If an existing pipeline has the same name it will be overwritten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:367158743199:pipeline/credit-default',\n",
       " 'ResponseMetadata': {'RequestId': 'b3a9bfdb-b1b2-4826-a75d-da5d737648f1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'b3a9bfdb-b1b2-4826-a75d-da5d737648f1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Thu, 04 Mar 2021 19:59:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-west-2:367158743199:pipeline/credit-default',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-west-2:367158743199:pipeline/credit-default/execution/ging0sljodj3',\n",
       " 'PipelineExecutionDisplayName': 'execution-1614888032576',\n",
       " 'PipelineExecutionStatus': 'Succeeded',\n",
       " 'CreationTime': datetime.datetime(2021, 3, 4, 20, 0, 32, 495000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 3, 4, 20, 4, 59, 15000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-west-2:367158743199:user-profile/d-cquuj3sidhuj/default-1613588520951',\n",
       "  'UserProfileName': 'default-1613588520951',\n",
       "  'DomainId': 'd-cquuj3sidhuj'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-west-2:367158743199:user-profile/d-cquuj3sidhuj/default-1613588520951',\n",
       "  'UserProfileName': 'default-1613588520951',\n",
       "  'DomainId': 'd-cquuj3sidhuj'},\n",
       " 'ResponseMetadata': {'RequestId': '2f5cf3c4-4a45-4201-860c-462f5bba7cae',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2f5cf3c4-4a45-4201-860c-462f5bba7cae',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '751',\n",
       "   'date': 'Thu, 04 Mar 2021 20:05:02 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_response = pipeline.start()\n",
    "\n",
    "start_response.wait()\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
