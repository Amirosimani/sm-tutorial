{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model\n",
    "\n",
    "In this second model, you will fix the gender imbalance in the dataset using SMOTE and train another model using XGBoost with hyperparameter tuning and Debugger. This model will also be saved to our registry and eventually approved for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn==0.7.0\n",
      "  Using cached imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Collecting scikit-learn>=0.23\n",
      "  Using cached scikit_learn-0.24.1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (1.18.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (1.4.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn, imbalanced-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.22.1\n",
      "    Uninstalling scikit-learn-0.22.1:\n",
      "      Successfully uninstalled scikit-learn-0.22.1\n",
      "Successfully installed imbalanced-learn-0.7.0 scikit-learn-0.24.1 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Set region, boto3 and SageMaker SDK variables¶\n",
    "\n",
    "#You can change this to a region of your choice\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "data_prefix                      -> 'sagemaker-tutorial/data'\n",
      "default_bucket                   -> 'sagemaker-us-east-1-367158743199'\n",
      "header                           -> ['LABEL', 'LIMIT_BAL', 'SEX', 'EDUCATION', 'MARRIA\n",
      "hyperparameters                  -> {'max_depth': '5', 'eta': '0.2', 'gamma': '4', 'mi\n",
      "local_data_dir                   -> '../data'\n",
      "local_processed_path             -> '../data/df_processed.csv'\n",
      "local_raw_path                   -> '../data/dataset.csv'\n",
      "mpg_name                         -> 'sagemaker-tutorial'\n",
      "prefix                           -> 'sagemaker-tutorial'\n",
      "s3_raw_data                      -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "test_data_uri                    -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "train_data_uri                   -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "training_job_name                -> 'sagemaker-xgboost-2021-04-06-02-12-29-010'\n",
      "validation_data_uri              -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n"
     ]
    }
   ],
   "source": [
    "# load stored variables\n",
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(local_processed_path)\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "One approach to addressing imbalanced datasets is to oversample the minority class. New examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=random_state)\n",
    "\n",
    "df_resampled, _ = sm.fit_resample(df, df['SEX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    18112\n",
       "1    18112\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the gender balance\n",
    "df_resampled['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df_resampled, test_size=0.2, random_state=random_state)\n",
    "X_train, X_val = train_test_split(df_resampled, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_processed_data = './processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(f'{local_data_dir}/train_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_data_dir}/train_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "train_res_data_uri = response\n",
    "%store train_res_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_val.to_csv(f'{local_data_dir}/validation_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_data_dir}/validation_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "validation_res_data_uri = response\n",
    "%store validation_res_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_test.to_csv(f'{local_data_dir}/test_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_data_dir}/test_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "test_res_data_uri = response\n",
    "%store test_res_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating XGBoost model with Hyperparameter Tunining and Debugger\n",
    "\n",
    "For SageMaker XGBoost training jobs, use the Debugger `CreateXgboostReport` rule to receive a comprehensive training report of the training progress and results. Following this guide, specify the CreateXgboostReport rule while constructing an XGBoost estimator. The `CreateXgboostReport` rule collects the following output tensors from your training job:\n",
    "\n",
    "* hyperparameters – Saves at the first step.\n",
    "* metrics – Saves loss and accuracy every 5 steps.\n",
    "* feature_importance – Saves every 5 steps.\n",
    "* predictions – Saves every 5 steps.\n",
    "* labels – Saves every 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "content_type = \"text/csv\"\n",
    "estimator_output_path = f's3://{default_bucket}/{prefix}/training_jobs'\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container,\n",
    "                                              hyperparameters=hyperparameters,\n",
    "                                              role=sagemaker.get_execution_role(),\n",
    "                                              instance_count=train_instance_count,\n",
    "                                              instance_type=train_instance_type,\n",
    "                                              volume_size=5,  # 5 GB\n",
    "                                              output_path=estimator_output_path,\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will tune four hyperparameters in this examples:\n",
    "\n",
    "* eta: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "* alpha: L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "* min_child_weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "* max_depth: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'max_depth': IntegerParameter(1, 10),\n",
    "                         'eta': ContinuousParameter(0, 1),\n",
    "                         'gamma': ContinuousParameter(0, 5),\n",
    "                        'alpha': ContinuousParameter(0, 2)\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: validation:auc and train:auc, and we elected to monitor validation:auc as you can see below. In this case, we only need to specify the metric name and do not need to provide regex. If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation:f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a HyperparameterTuner object, to which we pass:\n",
    "\n",
    "* The XGBoost estimator we created above\n",
    "* Our hyperparameter ranges\n",
    "* Objective metric name and definition\n",
    "* Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=4,\n",
    "                            max_parallel_jobs=2)\n",
    "\n",
    "# You can increase the number of jobs, etc. I set them to 10, 4 for the demo purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Hyperparameter Tuning job\n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling fit() function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Unknown variable 'training_smote_job_name'\n"
     ]
    }
   ],
   "source": [
    "# define the data type and paths to the training and validation datasets\n",
    "train_input = TrainingInput(train_data_uri, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(validation_data_uri, content_type=\"text/csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "tuner.fit({'train': train_input,\n",
    "                   'validation': validation_input\n",
    "                  }\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'training_smote_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "training_smote_job_name = tuner.best_training_job()\n",
    "%store training_smote_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model from estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_smote_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sagemaker-tutorial-xgboost-smote already exists.\n"
     ]
    }
   ],
   "source": [
    "model_2_name = f'{prefix}-xgboost-smote'\n",
    "\n",
    "\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_2_name)['Models']\n",
    "\n",
    "if not model_matches:\n",
    "    \n",
    "    model_2 = sagemaker_session.create_model_from_job(\n",
    "        name=model_2_name,\n",
    "        training_job_name=training_job_info['TrainingJobName'],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info['AlgorithmSpecification']['TrainingImage'])\n",
    "    %store model_2_name\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(f\"Model {model_2_name} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-tutorial-xgboost-smote'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:367158743199:artifact/46e81ac5bb720131c95259d4f2325499\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_info['InputDataConfig'][0]['DataSource']['S3DataSource']['S3Uri']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=training_data_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {training_data_artifact.artifact_arn}')\n",
    "else:\n",
    "    training_data_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainingData',\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type='Dataset',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:367158743199:artifact/c627bbcebccfc80216660daf15fda7ba\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=trained_model_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {model_artifact.artifact_arn}')\n",
    "else:\n",
    "    model_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainedModel',\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type='Model',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {model_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set artifact associations¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(TrialComponentName=tuner.best_training_job()+'-aws-training-job')\n",
    "trial_component_arn = trial_component['TrialComponentArn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists between arn:aws:sagemaker:us-east-1:367158743199:experiment-trial-component/sagemaker-xgboost-210406-0236-004-a3848c52-aws-training-job and arn:aws:sagemaker:us-east-1:367158743199:artifact/46e81ac5bb720131c95259d4f2325499.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Input artifacts\n",
    "input_artifacts = [training_data_artifact]\n",
    "\n",
    "for a in input_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='ContributedTo',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Associate {trial_component_arn} and {a.artifact_arn}: SUCCEESFUL\\n\")\n",
    "    except:\n",
    "        print(f\"Association already exists between {trial_component_arn} and {a.artifact_arn}.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists between arn:aws:sagemaker:us-east-1:367158743199:experiment-trial-component/sagemaker-xgboost-210406-0236-004-a3848c52-aws-training-job and arn:aws:sagemaker:us-east-1:367158743199:artifact/46e81ac5bb720131c95259d4f2325499.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output artifacts\n",
    "\n",
    "output_artifacts = [model_artifact]\n",
    "\n",
    "for artifact_arn in output_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='Produced',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Associate {trial_component_arn} and {a.artifact_arn}: SUCCEESFUL\\n\")\n",
    "    except:\n",
    "        print(f\"Association already exists between {trial_component_arn} and {a.artifact_arn}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package for the Second Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {'classification_metrics': {}}\n",
    "for metric in training_job_info['FinalMetricDataList']:\n",
    "    stat = {metric['MetricName']: {'value': metric['Value']}}\n",
    "    model_metrics_report['classification_metrics'].update(stat)\n",
    "    \n",
    "with open('training_metrics.json', 'w') as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "    \n",
    "metrics_s3_key = f\"{prefix}/training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename='training_metrics.json', Bucket=default_bucket, Key=metrics_s3_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'ModelQuality': {\n",
    "        'Statistics': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f's3://{default_bucket}/{prefix}/{metrics_s3_key}'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_spec ={    \n",
    "    \"InferenceSpecification\": {\n",
    "        \"Containers\" : [{\n",
    "            \"Image\": training_job_info['AlgorithmSpecification']['TrainingImage'],\n",
    "            \"ModelDataUrl\": training_job_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "        }],\n",
    "        \"SupportedTransformInstanceTypes\": [\"ml.m4.xlarge\"],\n",
    "        \"SupportedRealtimeInferenceInstanceTypes\": [\"ml.m4.xlarge\"],\n",
    "        \"SupportedContentTypes\": ['text/csv'],\n",
    "        \"SupportedResponseMIMETypes\": ['text/csv']\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# {'ModelDataUrl': }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register second model package to Model Package Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mp2_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mp_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageDescription': 'XGBoost classifier with SMOTE',\n",
    "    'ModelApprovalStatus': 'PendingManualApproval',\n",
    "    'ModelMetrics': model_metrics\n",
    "}\n",
    "\n",
    "mp_input_dict.update(inference_spec)\n",
    "mp2_response = sagemaker_boto_client.create_model_package(**mp_input_dict)\n",
    "mp2_arn = mp2_response['ModelPackageArn']\n",
    "%store mp2_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model package status: Completed\n"
     ]
    }
   ],
   "source": [
    "# Check status of model package creation¶\n",
    "\n",
    "mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp2_response['ModelPackageArn'])\n",
    "mp_status = mp_info['ModelPackageStatus']\n",
    "\n",
    "while mp_status not in ['Completed', 'Failed']:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(ModelPackageName=mp2_response['ModelPackageArn'])\n",
    "    mp_status = mp_info['ModelPackageStatus']\n",
    "    print(f'model package status: {mp_status}')\n",
    "print(f'model package status: {mp_status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View both models in the registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ModelPackageGroupName': 'sagemaker-tutorial',\n",
       "  'ModelPackageVersion': 5,\n",
       "  'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:367158743199:model-package/sagemaker-tutorial/5',\n",
       "  'ModelPackageDescription': 'XGBoost classifier with SMOTE',\n",
       "  'CreationTime': datetime.datetime(2021, 4, 6, 2, 50, 10, 620000, tzinfo=tzlocal()),\n",
       "  'ModelPackageStatus': 'Completed',\n",
       "  'ModelApprovalStatus': 'PendingManualApproval'},\n",
       " {'ModelPackageGroupName': 'sagemaker-tutorial',\n",
       "  'ModelPackageVersion': 4,\n",
       "  'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:367158743199:model-package/sagemaker-tutorial/4',\n",
       "  'ModelPackageDescription': 'XGBoost classifier to detect insurance fraud.',\n",
       "  'CreationTime': datetime.datetime(2021, 4, 6, 2, 32, 39, 455000, tzinfo=tzlocal()),\n",
       "  'ModelPackageStatus': 'Completed',\n",
       "  'ModelApprovalStatus': 'PendingManualApproval'},\n",
       " {'ModelPackageGroupName': 'sagemaker-tutorial',\n",
       "  'ModelPackageVersion': 3,\n",
       "  'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:367158743199:model-package/sagemaker-tutorial/3',\n",
       "  'ModelPackageDescription': 'XGBoost classifier with SMOTE',\n",
       "  'CreationTime': datetime.datetime(2021, 4, 2, 20, 16, 49, 511000, tzinfo=tzlocal()),\n",
       "  'ModelPackageStatus': 'Completed',\n",
       "  'ModelApprovalStatus': 'Approved'},\n",
       " {'ModelPackageGroupName': 'sagemaker-tutorial',\n",
       "  'ModelPackageVersion': 2,\n",
       "  'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:367158743199:model-package/sagemaker-tutorial/2',\n",
       "  'ModelPackageDescription': 'XGBoost classifier with SMOTE',\n",
       "  'CreationTime': datetime.datetime(2021, 4, 2, 2, 32, 30, 40000, tzinfo=tzlocal()),\n",
       "  'ModelPackageStatus': 'Completed',\n",
       "  'ModelApprovalStatus': 'Approved'},\n",
       " {'ModelPackageGroupName': 'sagemaker-tutorial',\n",
       "  'ModelPackageVersion': 1,\n",
       "  'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:367158743199:model-package/sagemaker-tutorial/1',\n",
       "  'ModelPackageDescription': 'XGBoost classifier to detect insurance fraud.',\n",
       "  'CreationTime': datetime.datetime(2021, 4, 1, 20, 59, 31, 765000, tzinfo=tzlocal()),\n",
       "  'ModelPackageStatus': 'Completed',\n",
       "  'ModelApprovalStatus': 'PendingManualApproval'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)['ModelPackageSummaryList']"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
