{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model\n",
    "\n",
    "In this second model, you will fix the gender imbalance in the dataset using SMOTE and train another model using XGBoost with hyperparameter tuning and Debugger. This model will also be saved to our registry and eventually approved for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn==0.7.0 in /opt/conda/lib/python3.7/site-packages (0.7.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (1.18.1)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn==0.7.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.23->imbalanced-learn==0.7.0) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# from sagemaker import clarify\n",
    "# from sagemaker.session import Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Set region, boto3 and SageMaker SDK variables¶\n",
    "\n",
    "#You can change this to a region of your choice\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "clarify_data_uri                 -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "data_prefix                      -> 'sagemaker-tutorial/data'\n",
      "default_bucket                   -> 'sagemaker-us-east-1-367158743199'\n",
      "feature_group_name               -> 'FG-flow-sm-tutorial-31-16-16-17-9f41d66b'\n",
      "hyperparameters                  -> {'max_depth': '5', 'eta': '0.2', 'gamma': '4', 'mi\n",
      "local_processed_data             -> '../sm-tutorial/02_build_train/processed_data'\n",
      "model_data                       -> 's3://sagemaker-us-east-1-367158743199/tf2-resnet-\n",
      "mpg_name                         -> 'sagemaker-tutorial'\n",
      "prefix                           -> 'sagemaker-tutorial'\n",
      "s3_raw_data                      -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "s3_train_data                    -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "s3_validation_data               -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "test_data_uri                    -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "train_data_uri                   -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n",
      "training_job_1_name              -> 'sagemaker-xgboost-2021-04-01-19-55-41-144'\n",
      "validation_data_uri              -> 's3://sagemaker-us-east-1-367158743199/sagemaker-t\n"
     ]
    }
   ],
   "source": [
    "# load stored variables\n",
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f'{local_processed_data}/df_processed.csv')\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE\n",
    "\n",
    "One approach to addressing imbalanced datasets is to oversample the minority class. New examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "df_resampled, _ = sm.fit_resample(df, df['SEX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    5214\n",
       "1.0    5214\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the gender balance\n",
    "df_resampled['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving data back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df_resampled, test_size=0.2, random_state=random_state)\n",
    "\n",
    "X_train, X_val = train_test_split(df_resampled, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local_processed_data = './processed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_train.to_csv(f'{local_processed_data}/train_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_processed_data}/train_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "train_res_data_uri = response\n",
    "%store train_res_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_val.to_csv(f'{local_processed_data}/validation_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_processed_data}/validation_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "validation_res_data_uri = response\n",
    "%store validation_res_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_res_data_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "X_test.to_csv(f'{local_processed_data}/test_res.csv', header=False, index=False)\n",
    "\n",
    "response = sagemaker_session.upload_data(f'{local_processed_data}/test_res.csv',\n",
    "                                         bucket=default_bucket, \n",
    "                                         key_prefix=data_prefix)\n",
    "test_res_data_uri = response\n",
    "%store test_res_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating XGBoost model with Hyperparameter Tunining and Debugger\n",
    "\n",
    "For SageMaker XGBoost training jobs, use the Debugger `CreateXgboostReport` rule to receive a comprehensive training report of the training progress and results. Following this guide, specify the CreateXgboostReport rule while constructing an XGBoost estimator. The `CreateXgboostReport` rule collects the following output tensors from your training job:\n",
    "\n",
    "* hyperparameters – Saves at the first step.\n",
    "* metrics – Saves loss and accuracy every 5 steps.\n",
    "* feature_importance – Saves every 5 steps.\n",
    "* predictions – Saves every 5 steps.\n",
    "* labels – Saves every 5 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "content_type = \"text/csv\"\n",
    "estimator_output_path = f's3://{default_bucket}/{prefix}/training_jobs'\n",
    "\n",
    "rules=[\n",
    "    Rule.sagemaker(rule_configs.create_xgboost_report())\n",
    "]\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgb_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container,\n",
    "                                              hyperparameters=hyperparameters,\n",
    "                                              role=sagemaker.get_execution_role(),\n",
    "                                              instance_count=train_instance_count,\n",
    "                                              instance_type=train_instance_type,\n",
    "                                              volume_size=5,  # 5 GB\n",
    "                                              output_path=estimator_output_path,\n",
    "                                              rules=rules\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will tune four hyperparameters in this examples:\n",
    "\n",
    "* eta: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "* alpha: L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "* min_child_weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "* max_depth: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'max_depth': IntegerParameter(1, 10),\n",
    "                         'eta': ContinuousParameter(0, 1),\n",
    "                         'gamma': ContinuousParameter(0, 5),\n",
    "                        'alpha': ContinuousParameter(0, 2)\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. Since we are using built-in XGBoost algorithm here, it emits two predefined metrics: validation:auc and train:auc, and we elected to monitor validation:auc as you can see below. In this case, we only need to specify the metric name and do not need to provide regex. If you bring your own algorithm, your algorithm emits metrics by itself. In that case, you'll need to add a MetricDefinition object here to define the format of those metrics through regex, so that SageMaker knows how to extract those metrics from your CloudWatch logs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation:f1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a HyperparameterTuner object, to which we pass:\n",
    "\n",
    "* The XGBoost estimator we created above\n",
    "* Our hyperparameter ranges\n",
    "* Objective metric name and definition\n",
    "* Tuning resource configurations such as Number of training jobs to run in total and how many training jobs can be run in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb_estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=10,\n",
    "                            max_parallel_jobs=4)\n",
    "\n",
    "# You can increase the number of jobs, etc. I set them to 10, 4 for the demo purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Hyperparameter Tuning job\n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling fit() function. After the hyperparameter tuning job is created, we can go to SageMaker console to track the progress of the hyperparameter tuning job until it is completed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................!\n",
      "Stored 'training_smote_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "# define the data type and paths to the training and validation datasets\n",
    "train_input = TrainingInput(train_data_uri, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(validation_data_uri, content_type=\"text/csv\")\n",
    "\n",
    "# execute the XGBoost training job\n",
    "tuner.fit({'train': train_input,\n",
    "                   'validation': validation_input\n",
    "                  }\n",
    "                   )\n",
    "\n",
    "training_smote_job_name = tuner.latest_training_job.job_name\n",
    "%store training_smote_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Debugger XGBoost Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_output_path = f\"{estimator_output_path}/{tuner.latest_tuning_job.name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-367158743199/sagemaker-tutorial/training_jobs/sagemaker-xgboost-210402-0144'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.estimator.Estimator at 0x7f07bb11e290>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_estimator.bes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
