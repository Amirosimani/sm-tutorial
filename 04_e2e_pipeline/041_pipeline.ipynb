{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import boto3\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, FeatureStoreOutput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set region, boto3 and SageMaker SDK variablesÂ¶\n",
    "\n",
    "#You can change this to a region of your choice\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))\n",
    "\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "account_id = boto3.client('sts').get_caller_identity()[\"Account\"]\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a SageMaker Pipeline to Automate All the Steps from Data Prep to Model Deployment\n",
    "Now that youve manually done each step in our machine learning workflow, you can create a pipeline which trains a new model, persists the model in SageMaker and then adds the model to the registry.\n",
    "\n",
    "### Pipeline parameters\n",
    "An important feature of SageMaker Pipelines is the ability to define the steps ahead of time, but be able to change the parameters to those steps at execution without having to re-define the pipeline. This can be achieved by using ParameterInteger, ParameterFloat or ParameterString to define a value upfront which can be modified when you call pipeline.start(parameters=parameters) later. Only certain parameters can be defined this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_param = ParameterString(\n",
    "    name=\"TrainingInstance\",\n",
    "    default_value=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"PendingManualApproval\"\n",
    ")\n",
    "\n",
    "deploy_model_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename='./preprocessing.py', Bucket=default_bucket, Key=f'{prefix}/code/preprocessing.py')\n",
    "\n",
    "create_dataset_script_uri = f's3://{default_bucket}/{prefix}/code/preprocessing.py'\n",
    "\n",
    "\n",
    "create_dataset_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name='credit-create-dataset',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name='CreateDataset',\n",
    "    processor=create_dataset_processor,\n",
    "    inputs=[ProcessingInput(\n",
    "                        source=s3_raw_data,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "    outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/output/train'),\n",
    "             ProcessingOutput(output_name='test_data',  source='/opt/ml/processing/output/test')],\n",
    "    job_arguments=[\"--train-test-split-ratio\", '0.8'],\n",
    "    code=create_dataset_script_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "content_type = \"text/csv\"\n",
    "job_name = f'XgboostTrain-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "training_job_output_path = f's3://{default_bucket}/{prefix}/training_jobs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spot training\n",
    "\n",
    "Managed Spot Training uses Amazon EC2 Spot instance to run training jobs instead of on-demand instances. You can specify which training jobs use spot instances and a stopping condition that specifies how long Amazon SageMaker waits for a job to run using Amazon EC2 Spot instances.\n",
    "\n",
    "This time in the pipeline, we will perform XGBoost training using Spot Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_spot_instances = False\n",
    "max_run = 3600\n",
    "max_wait = 7200 if use_spot_instances else None\n",
    "checkpoint_s3_uri = (f's3://{default_bucket}/{prefix}/checkpoints/{job_name}' if use_spot_instances\n",
    "                      else None)\n",
    "print(\"Checkpoint path:\", checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "xgb_estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container,\n",
    "                                              hyperparameters=best_job_hp,\n",
    "                                              role=sagemaker.get_execution_role(),\n",
    "                                              instance_count=train_instance_count,\n",
    "                                              instance_type=train_instance_type,\n",
    "                                              volume_size=5,  # 5 GB\n",
    "                                              output_path=training_job_output_path,\n",
    "                                              use_spot_instances=use_spot_instances,\n",
    "                                              max_run=max_run,\n",
    "                                              max_wait=max_wait,\n",
    "                                              checkpoint_s3_uri=checkpoint_s3_uri\n",
    "                                             )\n",
    "\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=job_name,\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train': TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "        content_type=\"csv\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Pre-Deployment Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name='credit-default-demo-pipeline-xgboost',\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"ModelPreDeployment\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Bias Metrics with Clarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify config\n",
    "bias_report_output_path = f's3://{default_bucket}/{prefix}/clarify-output/bias'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "\n",
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=create_dataset_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "    label='LABEL',\n",
    "    dataset_type='text/csv',\n",
    "    s3_output_path=bias_report_output_path)\n",
    "\n",
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[0],\n",
    "    facet_name='SEX',\n",
    "    facet_values_or_threshold=[1])\n",
    "\n",
    "analysis_config = bias_data_config.get_config()\n",
    "analysis_config.update(bias_config.get_config())\n",
    "analysis_config[\"methods\"] = {\"pre_training_bias\": {\"methods\": \"all\"}}\n",
    "\n",
    "clarify_config_dir = pathlib.Path('config')\n",
    "clarify_config_dir.mkdir(exist_ok=True)\n",
    "with open(clarify_config_dir / 'analysis_config.json', 'w') as f:\n",
    "    json.dump(analysis_config, f)\n",
    "\n",
    "s3_client.upload_file(Filename='config/analysis_config.json', Bucket=default_bucket,\n",
    "                      Key=f'{prefix}/clarify-config/analysis_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify processing step\n",
    "clarify_processor = sagemaker.processing.Processor(\n",
    "    base_job_name='fraud-detection-demo-clarify-processor',\n",
    "    image_uri=sagemaker.clarify.image_uris.retrieve(framework='clarify', region=region),\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.xlarge')\n",
    "\n",
    "clarify_step = ProcessingStep(\n",
    "    name=\"ClarifyProcessor\",\n",
    "    processor=clarify_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"analysis_config\",\n",
    "            source=f's3://{default_bucket}/{prefix}/clarify-config/analysis_config.json',\n",
    "            destination=\"/opt/ml/processing/input/config\"),\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            input_name=\"dataset\",\n",
    "            source=create_dataset_step.properties.ProcessingOutputConfig.Outputs['train_data'].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/input/data\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output/analysis.json\",\n",
    "            destination=bias_report_output_path,\n",
    "            output_name=\"analysis_result\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelMetrics(object):\n",
    "    \"\"\"Accepts model metrics parameters for conversion to request dict.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_statistics=None,\n",
    "            model_constraints=None,\n",
    "            model_data_statistics=None,\n",
    "            model_data_constraints=None,\n",
    "            bias=None,\n",
    "            explainability=None,\n",
    "    ):\n",
    "        \"\"\"Initialize a ``ModelMetrics`` instance and turn parameters into dict.\n",
    "        Args:\n",
    "            model_constraints (MetricsSource):\n",
    "            model_data_constraints (MetricsSource):\n",
    "            model_data_statistics (MetricsSource):\n",
    "            bias (MetricsSource):\n",
    "            explainability (MetricsSource):\n",
    "        \"\"\"\n",
    "        self.model_statistics = model_statistics\n",
    "        self.model_constraints = model_constraints\n",
    "        self.model_data_statistics = model_data_statistics\n",
    "        self.model_data_constraints = model_data_constraints\n",
    "        self.bias = bias\n",
    "        self.explainability = explainability\n",
    "\n",
    "    def _to_request_dict(self):\n",
    "        \"\"\"Generates a request dictionary using the parameters provided to the class.\"\"\"\n",
    "        model_metrics_request = {}\n",
    "\n",
    "        model_quality = {}\n",
    "        if self.model_statistics is not None:\n",
    "            model_quality[\"Statistics\"] = self.model_statistics._to_request_dict()\n",
    "        if self.model_constraints is not None:\n",
    "            model_quality[\"Constraints\"] = self.model_constraints._to_request_dict()\n",
    "        if model_quality:\n",
    "            model_metrics_request[\"ModelQuality\"] = model_quality\n",
    "\n",
    "        model_data_quality = {}\n",
    "        if self.model_data_statistics is not None:\n",
    "            model_data_quality[\"Statistics\"] = self.model_data_statistics._to_request_dict()\n",
    "        if self.model_data_constraints is not None:\n",
    "            model_data_quality[\"Constraints\"] = self.model_data_constraints._to_request_dict()\n",
    "        if model_data_quality:\n",
    "            model_metrics_request[\"ModelDataQuality\"] = model_data_quality\n",
    "\n",
    "        if self.bias is not None:\n",
    "            model_metrics_request[\"Bias\"] = {\"Report\": self.bias._to_request_dict()}\n",
    "            # model_metrics_request[\"Bias\"] = self.bias._to_request_dict()\n",
    "        if self.explainability is not None:\n",
    "            model_metrics_request[\"Explainability\"] = self.explainability._to_request_dict()\n",
    "        return model_metrics_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = ModelMetrics(\n",
    "    bias=sagemaker.model_metrics.MetricsSource(\n",
    "        s3_uri=clarify_step.properties.ProcessingOutputConfig.Outputs['analysis_result'].S3Output.S3Uri,\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if 'mpg_name' not in locals():\n",
    "    mpg_name = prefix\n",
    "    print(f'Model Package Group name: {mpg_name}')\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"XgboostRegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"xgboost-model-pipeline-\" + strftime('%d-%H-%M-%S', gmtime())\n",
    "deploy_model_script_uri = f's3://{default_bucket}/{prefix}/code/deploy_model.py'\n",
    "\n",
    "\n",
    "s3_client.upload_file(Filename='deploy_model.py', Bucket=default_bucket, Key=f'{prefix}/code/deploy_model.py')\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name='fraud-detection-demo-deploy-model',\n",
    "    sagemaker_session=sagemaker_session)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='DeployModel',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName,\n",
    "        \"--region\", region,\n",
    "        \"--endpoint-instance-type\", deploy_model_instance_type,\n",
    "        \"--endpoint-name\", endpoint_name],\n",
    "    code=deploy_model_script_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Pipeline Steps and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f'credit-default'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_instance_param,\n",
    "        model_approval_status],\n",
    "    steps=[\n",
    "        create_dataset_step,\n",
    "        train_step,\n",
    "        create_model_step,\n",
    "        clarify_step,\n",
    "        register_step,\n",
    "        deploy_step\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline definition to the SageMaker Pipeline service\n",
    "\n",
    "Note: If an existing pipeline has the same name it will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full pipeline will take up to 25 min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_response = pipeline.start()\n",
    "\n",
    "start_response.wait()\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "After running the demo, you should remove the resources which were created. You can also delete all the objects in the project's S3 directory by passing the keyword argument delete_s3_objects=True.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import delete_project_resources\n",
    "\n",
    "# delete_project_resources(\n",
    "#     sagemaker_boto_client=sagemaker_boto_client,\n",
    "#     endpoint_name=endpoint_name, \n",
    "#     pipeline_name=pipeline_name, \n",
    "#     mpg_name=mpg_name, \n",
    "#     prefix=prefix,\n",
    "#     delete_s3_objects=False,\n",
    "#     bucket_name=default_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
