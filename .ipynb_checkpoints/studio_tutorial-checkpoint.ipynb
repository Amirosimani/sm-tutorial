{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install -q scramp==1.2.2 awswrangler==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries including SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "from time import gmtime, strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update SageMaker SDK if necessary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker SDK version is good\n"
     ]
    }
   ],
   "source": [
    "if int(sagemaker.__version__.split('.')[0]) != 2:\n",
    "    !pip install sagemaker==2.24.1\n",
    "    print(\"Updating SageMakerVersion. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"SageMaker SDK version is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region = us-west-2\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "region = boto_session.region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "\n",
    "sagemaker_boto_client = boto_session.client('sagemaker')\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client)\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories in the SageMaker default bucket for this tutorial¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_bucket= sess.default_bucket() # Alterantively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-tutorial' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following code snippet to download the dataset to /data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./data’: File exists\n",
      "--2021-03-04 18:22:18--  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5539328 (5.3M) [application/x-httpd-php]\n",
      "Saving to: ‘./data/default_of_credit_card.xls’\n",
      "\n",
      "./data/default_of_c 100%[===================>]   5.28M  19.5MB/s    in 0.3s    \n",
      "\n",
      "2021-03-04 18:22:18 (19.5 MB/s) - ‘./data/default_of_credit_card.xls’ saved [5539328/5539328]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./data\n",
    "!wget -O ./data/default_of_credit_card.xls  https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default payment next month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...          0          0          0         0       689         0   \n",
       "1  ...       3272       3455       3261         0      1000      1000   \n",
       "2  ...      14331      14948      15549      1518      1500      1000   \n",
       "3  ...      28314      28959      29547      2000      2019      1200   \n",
       "4  ...      20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "0         0         0         0                           1  \n",
       "1      1000         0      2000                           1  \n",
       "2      1000      1000      5000                           0  \n",
       "3      1100      1069      1000                           0  \n",
       "4      9000       689       679                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change column names and save the file as .csv\n",
    "data_path = './data/default_of_credit_card.xls'\n",
    "\n",
    "df = pd.read_excel('./data/default_of_credit_card.xls', header=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                            0\n",
       "LIMIT_BAL                     0\n",
       "SEX                           0\n",
       "EDUCATION                     0\n",
       "MARRIAGE                      0\n",
       "AGE                           0\n",
       "PAY_0                         0\n",
       "PAY_2                         0\n",
       "PAY_3                         0\n",
       "PAY_4                         0\n",
       "PAY_5                         0\n",
       "PAY_6                         0\n",
       "BILL_AMT1                     0\n",
       "BILL_AMT2                     0\n",
       "BILL_AMT3                     0\n",
       "BILL_AMT4                     0\n",
       "BILL_AMT5                     0\n",
       "BILL_AMT6                     0\n",
       "PAY_AMT1                      0\n",
       "PAY_AMT2                      0\n",
       "PAY_AMT3                      0\n",
       "PAY_AMT4                      0\n",
       "PAY_AMT5                      0\n",
       "PAY_AMT6                      0\n",
       "default payment next month    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_features = df.drop(columns=['income', 'native-country'])\n",
    "\n",
    "# # removing trailing and heading whitespaces in column values\n",
    "# string_cols = df_features.select_dtypes(['object']).columns\n",
    "# df_features[string_cols] = df_features[string_cols].apply(lambda x: x.str.strip())\n",
    "\n",
    "# # remove special characters from native-country column\n",
    "# # df_features['native-country'] = df_features['native-country'].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x))\n",
    "\n",
    "\n",
    "# # one hot encoding is applied to prepare the data for feature store ingestion\n",
    "# df_features = pd.get_dummies(df_features)\n",
    "\n",
    "# df_features = df_features.fillna(0)\n",
    "# df_features = df_features.reset_index()\n",
    "\n",
    "timestamp = pd.to_datetime('now').timestamp()\n",
    "df['EVENT_TIME'] = timestamp\n",
    "\n",
    "# df['ID'] =  pd.to_numeric(df['ID'] , downcast='float64')\n",
    "# df['ID'] =df['ID'].astype(float)\n",
    "# df['ID'] = df['ID'].apply(np.uint64)\n",
    "\n",
    "df = df.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Amazon SageMaker built-in XGBoost algorithm, the label column needs to be the first column in the dataframe. Use the code below to make that change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(df)\n",
    "cols.insert(0, cols.pop(cols.index('default payment next month')))\n",
    "df = df.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"default payment next month\": \"LABEL\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to upload the data to S3. Also after running the cell below, you can continue the guide using the provided `./data/dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial/data/dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('./data/dataset.csv', index=False)\n",
    "\n",
    "response = sess.upload_data('./data/dataset.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create train and test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=.80, random_state=42)\n",
    "df_test = df.drop(df_train.index)\n",
    "\n",
    "# save train and test data locally\n",
    "df_train.to_csv('./data/train.csv', index=False)\n",
    "df_test.to_csv('./data/test.csv', index=False)\n",
    "\n",
    "# upload train and test data to s3\n",
    "response = sess.upload_data('./data/train.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "train_data_uri = response\n",
    "\n",
    "response = sess.upload_data('./data/test.csv', bucket=default_bucket, key_prefix=dataprefix)\n",
    "test_data_uri = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker Feature Store is a purpose-built repository where you can store and access features so it’s much easier to name, organize, and reuse them across teams. SageMaker Feature Store provides a unified store for features during training and real-time inference without the need to write additional code or create manual processes to keep features consistent. SageMaker Feature Store keeps track of the metadata of stored features (e.g. feature name or version number) so that you can query the features for the right attributes in batches or in real time using Amazon Athena, an interactive query service. SageMaker Feature Store also keeps features updated, because as new data is generated during inference, the single repository is updated so new features are always available for models to use during training and inference.\n",
    "\n",
    "A feature store consists of an offline componet stored in S3 and an online component stored in a low-latency database. The online database is optional, but very useful if you need supplemental features to be available at inference. In this section, we will create a feature groups for our Claims and Customers datasets. After inserting the claims and customer data into their respective feature groups, you need to query the offline store with Athena to build the training dataset.\n",
    "\n",
    "You can reference the [SageMaker Developer Guide](https://docs.aws.amazon.com/sagemaker/latest/dg/feature-store.html) for more information about the SageMaker Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurestore_runtime = boto_session.client(\n",
    "    service_name='sagemaker-featurestore-runtime', \n",
    "    region_name=region\n",
    ")\n",
    "\n",
    "feature_store_session = sagemaker.Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_boto_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the feature groups\n",
    "\n",
    "When you set up your feature groups, you need to customize the feature names with a unique name and set up each feature group with the FeatureGroup class. \n",
    "\n",
    "\n",
    "The datatype for each feature is set by passing a dataframe and inferring the proper datatype. Feature data types can also be set via a config variable, but it will have to match the correspongin Python data type in the Pandas dataframe when it's ingested to the Feature Group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_name = f'credit-' + strftime('%d-%H-%M-%S', gmtime())\n",
    "\n",
    "credit_feature_group = FeatureGroup(\n",
    "    name=fg_name, \n",
    "    sagemaker_session=feature_store_session)\n",
    "\n",
    "# You can now load the feature definitions by passing a data frame containing the feature data.\n",
    "credit_feature_group.load_feature_definitions(data_frame=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the feature groups¶\n",
    "\n",
    "You must tell the Feature Group which columns in the dataframe correspond to the required record indentifier and event time features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credit-04-18-22-25 is the feature group name in use\n"
     ]
    }
   ],
   "source": [
    "print(f\"{fg_name} is the feature group name in use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, you use the create function to create the feature group. The following code shows all of the available parameters. The online store is not created by default, so you must set this as True if you want to enable it. The s3_uri is the S3 bucket location of your offline store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Using s3://{default_bucket}/{prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "Waiting for Feature Group Creation\n",
      "FeatureGroup credit-04-18-22-25 successfully created.\n"
     ]
    }
   ],
   "source": [
    "# record identifier and event time feature names\n",
    "record_identifier_feature_name = 'ID'\n",
    "event_time_feature_name = 'EVENT_TIME'\n",
    "\n",
    "\n",
    "# check if the feature groups is created successfully \n",
    "def wait_for_feature_group_creation_complete(feature_group):\n",
    "    status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group Creation\")\n",
    "        time.sleep(5)\n",
    "        status = feature_group.describe().get(\"FeatureGroupStatus\")\n",
    "    if status != \"Created\":\n",
    "        raise RuntimeError(f\"Failed to create feature group {feature_group.name}\")\n",
    "    print(f\"FeatureGroup {feature_group.name} successfully created.\")\n",
    "    \n",
    "\n",
    "print(f\"\\n Using s3://{default_bucket}/{prefix}\")\n",
    "credit_feature_group.create(\n",
    "    s3_uri=f\"s3://{default_bucket}/{prefix}\",\n",
    "    record_identifier_name=record_identifier_feature_name,\n",
    "    event_time_feature_name=event_time_feature_name,\n",
    "    role_arn=sagemaker_role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "wait_for_feature_group_creation_complete(feature_group=credit_feature_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-west-2:367158743199:feature-group/credit-04-18-22-25',\n",
       " 'FeatureGroupName': 'credit-04-18-22-25',\n",
       " 'RecordIdentifierFeatureName': 'ID',\n",
       " 'EventTimeFeatureName': 'EVENT_TIME',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'LABEL', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'ID', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'LIMIT_BAL', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'SEX', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'EDUCATION', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'MARRIAGE', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'AGE', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_0', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT1', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'BILL_AMT6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT1', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT2', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT3', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT4', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT5', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'PAY_AMT6', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'EVENT_TIME', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 3, 4, 18, 22, 27, 359000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-west-2-367158743199/sagemaker-tutorial'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'credit-04-18-22-25-1614882147',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'RoleArn': 'arn:aws:iam::367158743199:role/service-role/AmazonSageMaker-ExecutionRole-20210217T120320',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'ResponseMetadata': {'RequestId': '0113d94e-094e-49d8-8fd5-5e885a3071c6',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '0113d94e-094e-49d8-8fd5-5e885a3071c6',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2635',\n",
       "   'date': 'Thu, 04 Mar 2021 18:22:42 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_feature_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest records into the Feature Groups\n",
    "\n",
    "After the Feature Groups have been created, we can put data into each store by using the PutRecord API. This API can handle high TPS and is designed to be called by different streams. The data from all of these Put requests is buffered and written to s3 in chunks. The files will be written to the offline store within a few minutes of ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'credit_table' in locals():\n",
    "    print(\"You may have already ingested the data into your Feature Groups. If you'd like to do this again, you can run the ingest methods outside of the 'if/else' statement.\")\n",
    "\n",
    "else:\n",
    "    credit_feature_group.ingest(\n",
    "    data_frame=df, max_workers=5, wait=True\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using XGBoost built-in algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and test datasets have been persisted in S3, you can start training a model by defining which SageMaker Estimator you'd like to use. For this guide, you will use the XGBoost built-in algorithm to build an XGBoost training container as shown in the following code example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-04 18:24:12 Starting - Starting the training job...\n",
      "2021-03-04 18:24:36 Starting - Launching requested ML instancesProfilerReport-1614882252: InProgress\n",
      "......\n",
      "2021-03-04 18:25:44 Starting - Preparing the instances for training......\n",
      "2021-03-04 18:26:38 Downloading - Downloading input data...\n",
      "2021-03-04 18:27:15 Training - Training image download completed. Training in progress.\n",
      "2021-03-04 18:27:15 Uploading - Uploading generated training model.\u001b[34m[2021-03-04 18:27:10.682 ip-10-0-241-192.us-west-2.compute.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 24001 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 6001 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.17570#011validation-error:0.18763\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.17462#011validation-error:0.18530\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.17453#011validation-error:0.18514\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.17416#011validation-error:0.18497\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.17333#011validation-error:0.18480\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.17278#011validation-error:0.18547\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.17237#011validation-error:0.18530\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.17241#011validation-error:0.18547\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.17291#011validation-error:0.18564\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.17274#011validation-error:0.18480\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.17220#011validation-error:0.18414\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.17195#011validation-error:0.18280\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.17166#011validation-error:0.18364\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.17174#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.17120#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.17120#011validation-error:0.18364\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.17070#011validation-error:0.18314\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.17058#011validation-error:0.18264\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.17053#011validation-error:0.18297\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.17083#011validation-error:0.18247\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.17024#011validation-error:0.18280\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.17003#011validation-error:0.18330\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.16970#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.16987#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.16953#011validation-error:0.18364\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.16933#011validation-error:0.18364\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.16887#011validation-error:0.18397\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.16858#011validation-error:0.18230\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.16870#011validation-error:0.18197\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.16841#011validation-error:0.18214\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.16824#011validation-error:0.18264\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.16845#011validation-error:0.18314\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.16841#011validation-error:0.18330\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.16828#011validation-error:0.18247\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.16833#011validation-error:0.18264\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.16812#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.16787#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.16778#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.16778#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.16770#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.16703#011validation-error:0.18280\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.16674#011validation-error:0.18247\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.16678#011validation-error:0.18314\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.16624#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.16624#011validation-error:0.18347\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.16587#011validation-error:0.18414\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.16566#011validation-error:0.18497\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.16583#011validation-error:0.18480\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.16533#011validation-error:0.18447\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.16566#011validation-error:0.18397\u001b[0m\n",
      "\n",
      "2021-03-04 18:27:38 Completed - Training job completed\n",
      "Training seconds: 57\n",
      "Billable seconds: 57\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"50\"}\n",
    "\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "model_prefix = 'xgboost'\n",
    "output_path = 's3://{}/{}/output'.format(default_bucket, prefix)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"text/csv\"\n",
    "train_input = TrainingInput(train_data_uri, content_type=content_type)\n",
    "validation_input = TrainingInput(test_data_uri, content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model lineage with artifacts and associations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker ML Lineage Tracking creates and stores information about the steps of a machine learning (ML) workflow from data preparation to model deployment. With the tracking information you can reproduce the workflow steps, track model and dataset lineage, and establish model governance and audit standards. With SageMaker Lineage Tracking data scientists and model builders can do the following:\n",
    "\n",
    "* Keep a running history of model discovery experiments.\n",
    "* Establish model governance by tracking model lineage artifacts for auditing and compliance verification.\n",
    "* Clone and rerun workflows to experiment with what-if scenarios while developing models.\n",
    "* Share a workflow that colleagues can reproduce and enhance (for example, while collaborating on solving a business problem).\n",
    "* Clone and rerun workflows with additional debugging or logging routines, or new input variations for troubleshooting issues in production models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the xgb_estimator object retains much the data we need to learn about how the model was trained, it is, in fact, an ephermeral object which SageMaker does not persist and cannot be re-instantiated at a later time. Although we lose some of its convieneces once it is gone, we can still get back all the data we need by accessing the training jobs it once created.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_1_info = sagemaker_boto_client.describe_training_job(TrainingJobName=estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-west-2:367158743199:artifact/900bea73068df4bfc58a8391770aa9bd\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_1_info['InputDataConfig'][0]['DataSource']['S3DataSource']['S3Uri']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=training_data_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {training_data_artifact.artifact_arn}')\n",
    "else:\n",
    "    training_data_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainingData',\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type='Dataset',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-west-2:367158743199:artifact/a4dbd7aa3a1155dfb51b7f56d97b328c\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_1_info['ModelArtifacts']['S3ModelArtifacts']\n",
    "\n",
    "matching_artifacts = list(sagemaker.lineage.artifact.Artifact.list(\n",
    "    source_uri=trained_model_s3_uri,\n",
    "    sagemaker_session=sagemaker_session))\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f'Using existing artifact: {model_artifact.artifact_arn}')\n",
    "else:\n",
    "    model_artifact = sagemaker.lineage.artifact.Artifact.create(\n",
    "        artifact_name='TrainedModel',\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type='Model',\n",
    "        sagemaker_session=sagemaker_session)\n",
    "    print(f'Create artifact {model_artifact.artifact_arn}: SUCCESSFUL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set artifact associations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_1_name = training_job_1_info['TrainingJobName']\n",
    "trial_component = sagemaker_boto_client.describe_trial_component(TrialComponentName=training_job_1_name+'-aws-training-job')\n",
    "trial_component_arn = trial_component['TrialComponentArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists with DataSet\n"
     ]
    }
   ],
   "source": [
    "input_artifacts = [training_data_artifact]\n",
    "\n",
    "for a in input_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='ContributedTo',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Association with {a.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {a.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association with Model: SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "output_artifacts = [model_artifact]\n",
    "\n",
    "for a in output_artifacts:\n",
    "    try:\n",
    "        sagemaker.lineage.association.Association.create(\n",
    "            source_arn=a.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type='Produced',\n",
    "            sagemaker_session=sagemaker_session)\n",
    "        print(f\"Association with {a.artifact_type}: SUCCESSFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {a.artifact_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model for bias with Clarify\n",
    "\n",
    "Amazon SageMaker Clarify helps improve your machine learning (ML) models by detecting potential bias and helping explain the predictions that models make. It helps you identify various types of bias in pretraining data and in posttraining that can emerge during model training or when the model is in production. SageMaker Clarify helps explain how these models make predictions using a feature attribution approach. It also monitors inferences models make in production for bias or feature attribution drift. The fairness and explainability functionality provided by SageMaker Clarify provides components that help AWS customers build less biased and more understandable machine learning models. It also provides tools to help you generate model governance reports which you can use to inform risk and compliance teams, and external regulators.\n",
    "\n",
    "You can reference the SageMaker Developer Guide for more information about SageMaker Clarify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sagemaker-tutorial-xgboost-pre-smote already exists.\n"
     ]
    }
   ],
   "source": [
    "# Create model from estimator\n",
    "\n",
    "model_1_name = f'{prefix}-xgboost-pre-smote'\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_1_name)['Models']\n",
    "\n",
    "if not model_matches:\n",
    "    \n",
    "    model_1 = sagemaker_session.create_model_from_job(\n",
    "        name=model_1_name,\n",
    "        training_job_name=training_job_1_info['TrainingJobName'],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_1_info['AlgorithmSpecification']['TrainingImage'])\n",
    "else:\n",
    "    \n",
    "    print(f\"Model {model_1_name} already exists.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for data set bias and model bias\n",
    "With SageMaker, we can check for pre-training and post-training bias. Pre-training metrics show pre-existing bias in that data, while post-training metrics show bias in the predictions from the model. Using the SageMaker SDK, we can specify which groups we want to check bias across and which metrics we'd like to show.\n",
    "\n",
    "To run the full Clarify job, you must un-comment the code in the cell below. Running the job will take ~15 minutes. If you wish to save time, you can view the results in the next cell after which loads a pre-generated output if no bias job was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_report_1_output_path = f's3://{default_bucket}/{prefix}/clarify-output/bias_1'\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "\n",
    "train_cols = wr.s3.read_csv(training_data_s3_uri).columns.to_list()\n",
    "\n",
    "clarify_processor = sagemaker.clarify.SageMakerClarifyProcessor(\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=train_instance_type,\n",
    "    sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_data_config = sagemaker.clarify.DataConfig(\n",
    "    s3_data_input_path=train_data_uri,\n",
    "    s3_output_path=bias_report_1_output_path,\n",
    "    label='LABEL',\n",
    "    headers=train_cols,\n",
    "    dataset_type='text/csv')\n",
    "\n",
    "model_config = sagemaker.clarify.ModelConfig(\n",
    "    model_name=model_1_name,\n",
    "    instance_type=train_instance_type,\n",
    "    instance_count=1,\n",
    "    accept_type='text/csv',\n",
    "    content_type='text/csv')\n",
    "\n",
    "predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `BiasConfig` to provide information on which columns contain the facets (sensitive groups, Sex), what the sensitive features (facet_values_or_threshold) might be, and what the desirable outcomes are (label_values_or_threshold).\n",
    "\n",
    "You can run both the pretraining and posttraining analysis in the processing job at the same time with run_bias()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  Clarify-Bias-2021-03-04-18-28-23-739\n",
      "Inputs:  [{'InputName': 'dataset', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-367158743199/sagemaker-tutorial/data/train.csv', 'LocalPath': '/opt/ml/processing/input/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'analysis_config', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-367158743199/Clarify-Bias-2021-03-04-18-28-23-739/input/analysis_config/analysis_config.json', 'LocalPath': '/opt/ml/processing/input/config', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'analysis_result', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-367158743199/sagemaker-tutorial/clarify-output/bias_1', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "....................................\u001b[34mINFO:sagemaker-clarify-processing:Starting SageMaker Clarify Processing job\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis config path: /opt/ml/processing/input/config\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Analysis result path: /opt/ml/processing/output\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is algo-1.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:This host is the leader.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Number of hosts in the cluster is 1.\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Running Python / Pandas based analyzer.\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Dataset uri: /opt/ml/processing/input/data\u001b[0m\n",
      "\u001b[34mINFO:analyzer.data_loading.data_loader_util:Dataset type: text/csv\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Spinning up shadow endpoint\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Creating endpoint-config with name sagemaker-clarify-endpoint-config-1614882849-c9cd\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Creating endpoint sagemaker-clarify-endpoint-1614882849-548c\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:======================================\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Calculating post-training bias metrics\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:======================================\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-clarify-processing:Getting predictions from the endpoint\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Checking endpoint status\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Endpoint is in service after 541 seconds\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Prediction batch size is initialized with 38597\u001b[0m\n",
      "\u001b[34mINFO:analyzer.prediction_util:We assume a prediction above 0.500 indicates 1 and below or equal indicates 0.\u001b[0m\n",
      "\u001b[34mINFO:smclarify.bias.metrics.common:data uniqueness fraction: 8.333333333333333e-05\u001b[0m\n",
      "\u001b[34mINFO:analyzer.system_util:exit_message: Customer Error: left side of interval must be <= right side\u001b[0m\n",
      "\u001b[34mERROR:analyzer.system_util:Errors occurred when analyzing your data. Please check CloudWatch logs for more details.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/analyzer/analysis_driver.py\", line 278, in main\n",
      "    violations: int = run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/analyzer/analysis_driver.py\", line 191, in run\n",
      "    summary = run_pandas(configs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/analyzer/analysis_driver.py\", line 131, in run_pandas\n",
      "    predicted_labels=predicted_labels,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/analyzer/bias_analyzer.py\", line 85, in analyze\n",
      "    predicted_labels=predicted_labels,\n",
      "  File \"/usr/local/lib/python3.7/site-packages/analyzer/posttraining_bias_analyzer.py\", line 40, in generate_bias_report\n",
      "    label_val_or_threshold = label_value_or_threshold(labels, label_values)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/smclarify/bias/report.py\", line 238, in label_value_or_threshold\n",
      "    _, value_or_threshold = _positive_label_index(data=label_series, positive_values=positive_values)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/smclarify/bias/report.py\", line 216, in _positive_label_index\n",
      "    data_interval_indices = _interval_index(data, positive_values)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/smclarify/bias/report.py\", line 163, in _interval_index\n",
      "    return pd.IntervalIndex.from_breaks(threshold_intervals)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/core/indexes/interval.py\", line 271, in from_breaks\n",
      "    breaks, closed=closed, copy=copy, dtype=dtype\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/core/arrays/interval.py\", line 314, in from_breaks\n",
      "    return cls.from_arrays(breaks[:-1], breaks[1:], closed, copy=copy, dtype=dtype)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/core/arrays/interval.py\", line 387, in from_arrays\n",
      "    left, right, closed, copy=copy, dtype=dtype, verify_integrity=True\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/core/arrays/interval.py\", line 246, in _simple_new\n",
      "    result._validate()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/pandas/core/arrays/interval.py\", line 494, in _validate\n",
      "    raise ValueError(msg)\u001b[0m\n",
      "\u001b[34mValueError: left side of interval must be <= right side\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Deleting endpoint configuration with name: sagemaker-clarify-endpoint-config-1614882849-c9cd\u001b[0m\n",
      "\u001b[34mINFO:sagemaker:Deleting endpoint with name: sagemaker-clarify-endpoint-1614882849-548c\u001b[0m\n",
      "\u001b[34mINFO:analyzer.predictor:Model endpoint delivered 0.62986 requests per second and a total of 1 requests over 2 seconds\u001b[0m\n",
      "\u001b[34m---------!\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job Clarify-Bias-2021-03-04-18-28-23-739: Failed. Reason: ClientError: left side of interval must be <= right side\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e4319d3776b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel_predicted_label_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpre_training_methods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     post_training_methods='all')\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mclarify_bias_job_1_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclarify_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/clarify.py\u001b[0m in \u001b[0;36mrun_bias\u001b[0;34m(self, data_config, bias_config, model_config, model_predicted_label_config, pre_training_methods, post_training_methods, wait, logs, job_name, kms_key)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjob_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_from_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clarify-Bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalysis_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkms_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     def run_explainability(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/clarify.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, data_config, analysis_config, wait, logs, job_name, kms_key)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                 \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             )\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extend_processing_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=W0613\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \"\"\"\n\u001b[1;32m    866\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_processing_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3777\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProcessingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3778\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3779\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3265\u001b[0m                 ),\n\u001b[1;32m   3266\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m             )\n\u001b[1;32m   3269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job Clarify-Bias-2021-03-04-18-28-23-739: Failed. Reason: ClientError: left side of interval must be <= right side\n"
     ]
    }
   ],
   "source": [
    "bias_config = sagemaker.clarify.BiasConfig(\n",
    "    label_values_or_threshold=[2],\n",
    "    facet_name='SEX',\n",
    "    facet_values_or_threshold=[1])\n",
    "\n",
    "\n",
    "clarify_processor.run_bias(\n",
    "    data_config=bias_data_config,\n",
    "    bias_config=bias_config,\n",
    "    model_config=model_config,\n",
    "    model_predicted_label_config=predictions_config,\n",
    "    pre_training_methods='all',\n",
    "    post_training_methods='all')\n",
    "\n",
    "clarify_bias_job_1_name = clarify_processor.latest_job.name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View results of Clarify job\n",
    "Running Clarify on your dataset or model can take ~15 minutes. Once it is done, you can view the results in Studio or download them from the bias_report_output_path S3 bucket.\n",
    "\n",
    "If you don't have time to run the job, you can view the pre-generated results included with this demo. Otherwise, you can run the job by un-commenting the code in the cell above.\n",
    "\n",
    "{PLACEHOLDER https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.ipynb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'clarify_bias_job_name' in locals():\n",
    "    s3_client.download_file(Bucket=bucket, Key=f'{prefix}/clarify-output/bias-1/analysis.json', Filename='clarify_output/bias_1/analysis.json')\n",
    "    print(f'Downloaded analysis from previous Clarify job: {clarify_bias_job_name}')\n",
    "else:\n",
    "    print(f'Loading pre-generated analysis file...')\n",
    "\n",
    "with open('clarify_output/bias_1/analysis.json', 'r') as f:\n",
    "        bias_analysis = json.load(f)\n",
    "\n",
    "results = bias_analysis['pre_training_bias_metrics']['facets']['customer_gender_female'][0]['metrics'][1]\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deposit Model and Lineage in SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained and its artifacts properly associated, the next step is to save the model in a registry for future reference and possible deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package Group\n",
    "A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mpg_name' (str)\n",
      "Model Package Group name: sagemaker-tutorial\n"
     ]
    }
   ],
   "source": [
    "if 'mpg_name' not in locals():\n",
    "    mpg_name = prefix\n",
    "    %store mpg_name\n",
    "    print(f'Model Package Group name: {mpg_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    'ModelPackageGroupName': mpg_name,\n",
    "    'ModelPackageGroupDescription': 'Insurance claim fraud detection'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model Package Group sagemaker-tutorial: SUCCESSFUL\n",
      "Stored 'mpg_name' (str)\n"
     ]
    }
   ],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)['ModelPackageGroupSummaryList']\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f'Using existing Model Package Group: {mpg_name}')\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f'Create Model Package Group {mpg_name}: SUCCESSFUL')\n",
    "    %store mpg_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package for trained model¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-367158743199/sagemaker-tutorial/training_jobs/sagemaker-xgboost-2021-03-04-18-24-12-755/training_metrics.json'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and upload a metrics report¶\n",
    "\n",
    "model_metrics_report = {'classification_metrics': {}}\n",
    "for metric in training_job_1_info['FinalMetricDataList']:\n",
    "    stat = {metric['MetricName']: {'value': metric['Value']}}\n",
    "    model_metrics_report['classification_metrics'].update(stat)\n",
    "    \n",
    "with open('./data/training_metrics.json', 'w') as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "    \n",
    "metrics_prefix = f\"{prefix}/training_jobs/{training_job_1_info['TrainingJobName']}\"\n",
    "\n",
    "\n",
    "sess.upload_data('./data/training_metrics.json', bucket=default_bucket, key_prefix=metrics_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the inference spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inf import InferenceSpecification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_1_info['AlgorithmSpecification']['TrainingImage'],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=['text/csv'],\n",
    "    supported_mime_types=['text/csv'])\n",
    "\n",
    "\n",
    "mp_inference_spec['InferenceSpecification']['Containers'][0]['ModelDataUrl'] = training_job_1_info['ModelArtifacts']['S3ModelArtifacts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-51-c4968adb67c0>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-51-c4968adb67c0>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    },\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    'ModelQuality': {\n",
    "        'Statistics': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f's3://{bucket}/{prefix}/{metrics_s3_key}'\n",
    "        }\n",
    "    },\n",
    "    'Bias': {\n",
    "        'Report': {\n",
    "            'ContentType': 'application/json',\n",
    "            'S3Uri': f'{bias_report_1_output_path}/analysis.json'\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
